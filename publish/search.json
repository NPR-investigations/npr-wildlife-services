[
  {
    "objectID": "etl/4.transform_json_to_tabular.html",
    "href": "etl/4.transform_json_to_tabular.html",
    "title": "Analyze the jsonl",
    "section": "",
    "text": "library(tidyverse)\nlibrary(jsonlite)\nlibrary(here)\nlibrary(glue)",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Analyze the jsonl"
    ]
  },
  {
    "objectID": "etl/4.transform_json_to_tabular.html#reshape-json",
    "href": "etl/4.transform_json_to_tabular.html#reshape-json",
    "title": "Analyze the jsonl",
    "section": "Reshape json",
    "text": "Reshape json\n\narrange_matrix &lt;- function(lst, i) {\n  # Extract row and column indices\n  positions &lt;- sapply(names(lst), function(x) as.numeric(unlist(strsplit(gsub(\"[^0-9,]\", \"\", x), \",\"))))\n  rows &lt;- max(positions[1, ]) + 1\n  cols &lt;- max(positions[2, ]) + 1\n\n  # Create an empty matrix with the determined dimensions\n  mat &lt;- matrix(NA, nrow = rows, ncol = cols)\n  \n  # Populate the matrix using the keys and values from the list\n  for (key in names(lst)) {\n    position &lt;- as.numeric(unlist(strsplit(gsub(\"[^0-9,]\", \"\", key), \",\"))) + 1\n    mat[position[1], position[2]] &lt;- lst[[key]]\n\n}\n\n\n   # Convert the matrix to a dataframe\n   df &lt;- as.data.frame(mat, stringsAsFactors = FALSE)\n\n   \n   if(ncol(df) == 2){\n\n    df = df  %&gt;% \n      rename(column = V1, value = V2) %&gt;% \n      mutate(entry = i) %&gt;% \n      mutate(correct = TRUE)\n\n\n   } else {\n\n    df = df  %&gt;% \n      rename(column = V1) %&gt;% \n      mutate(entry = i) %&gt;% \n      mutate(correct = FALSE)\n\n\n\n\n\n   }\n\n   \n\n   return(df)\n  \n \n}\n\n\nfoia_json_to_df = function(path, label){\n\n  con &lt;- file(path, \"r\")\n\n# Initialize an empty list to store the data\njson_data &lt;- list()\n\n# Read and parse the file line by line\nwhile (length(line &lt;- readLines(con, n = 1, warn = FALSE)) &gt; 0) {\n  parsed_line &lt;- fromJSON(line)\n  json_data &lt;- append(json_data, list(parsed_line))\n}\n\n# Close the connection\nclose(con)\n\ndf = tibble()\n\nfor(i in seq(length(json_data))){\n\n   temp =  arrange_matrix(json_data[[i]], i)\n   df = bind_rows(df, temp)\n}\n\ndf = df  %&gt;% \n  mutate(file = label)\n\nreturn(df)\n\n\n}",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Analyze the jsonl"
    ]
  },
  {
    "objectID": "etl/4.transform_json_to_tabular.html#cleaning-functions",
    "href": "etl/4.transform_json_to_tabular.html#cleaning-functions",
    "title": "Analyze the jsonl",
    "section": "Cleaning functions",
    "text": "Cleaning functions\n\nclean_parsed = function(df){\n\n  phrases_to_remove &lt;- c(\"\\\\(b\\\\) \\\\(6\\\\)\", \"\\\\(b\\\\) 6\\\\)\" ,\"\\\\(0\\\\) \\\\(6\\\\) \\\\(0\\\\) \\\\(35 \\\\(a\\\\)\",\n\"b \\\\(6\", \"\\\\(o\\\\) \\\\(6\\\\)\",\n\"\\\\(0\\\\) \\\\(6\\\\)\\\\, \\\\(0\\\\) \\\\(35 \\\\(a\\\\)\",\n\"\\\\!\",\n\"\\\\?\",\n\"control\",  \n\"entered/edited by lori warnell\", \n\"control\",  \n\"edit\", \n\"flag\", \n\"sampling\",  \n\"disposition\",  \n\"technical\", \n\"assistance\", \n\"aerial\", \n\"direct\", \n\"entered/edited by dalin tidwell\", \n\"entered/ed by john steuber\",\n\"entered/ed by dalin tidwell\", \n \"≤ back to report tab test details 05-03-2023\",\n \"&lt; back to report tab test details 05-03-2023\",\n \"≤ back to report tab\",\n  \"entered/edited by dalin tidwell\",\n  \" d \",\n  \" z \",\n  \"8\")\n\n  df_clean = df  %&gt;% \n\n    mutate(column = str_squish(column), value = str_squish(value)) %&gt;% \n# Get rid of blank cells \n# ==========================\nfilter(!(is.na(column) == TRUE & is.na(value) == TRUE)) %&gt;% \nmutate(\n    column = case_when(\n      is.na(column) & grepl(\"^Activity:\", value) ~ \"Activity:\",\n      TRUE ~ column\n    ),\n    value = case_when(\n      column == \"Activity:\" & grepl(\"^Activity:\", value) ~ sub(\"^Activity: \", \"\", value),\n      TRUE ~ value\n    )\n  ) %&gt;% \n# When there is just a number followed by period and worktask in other column combine\n# =====================================================================================\nmutate(column = case_when(\n\n  str_detect(column, \"\\\\b\\\\d+\\\\.\\\\s*\") & str_detect(value, \"WorkTask\") ~ paste0(column, \" \", value),\nTRUE ~ column)) %&gt;% \n# Separate activity number\n# ===================\n mutate(\n    value = case_when(\n      grepl(\"^Activity: (\\\\d+)\", column) ~ {\n        activity_num &lt;- sub(\"^Activity: (\\\\d+).*\", \"\\\\1\", column)\n        paste0(activity_num, \" \", value)\n      },\n      TRUE ~ value\n    ), \n    column = case_when(\n      grepl(\"^Activity: (\\\\d+)\", column) ~ \"Activity:\",\n      TRUE ~ column\n\n    )\n  ) %&gt;% \n  # Make flagged comments a single column\n  # ====================================\n  mutate(\n    value = case_when(\n      str_detect(column, \"FlaggedX\") | str_detect(value, \"FlaggedX\") ~ paste0(column, \" \", value),\n      TRUE ~ value\n    ),\n    column = case_when(\n      str_detect(column, \"FlaggedX\") | str_detect(value, \"FlaggedX\") ~ \"Flagged:\",\n      TRUE ~ column\n\n    )\n  # Combine columns that overlapped page\n  # ======================================\n  ) %&gt;% \n  summarize(value = paste(value, collapse = \", \"), .by = c(entry, column, file)) %&gt;% \n  mutate(lag_column = lag(column)) %&gt;% \n  mutate(column = case_when(\n    is.na(column) == TRUE ~ lag_column,\n    TRUE ~ column\n  )) %&gt;% \n  summarize(value = paste(value, collapse = \"| \"), .by = c(entry, column, file)) %&gt;% \n  # Standardize column names\n  # =========================\n  mutate(column = tolower(str_squish(column))) %&gt;% \n  ## Fix Worktask\n  # ===================\n  mutate(column = str_replace_all(column, \"work task\", \"worktask\")) %&gt;% \n  mutate(column = case_when(str_detect(column, \"worktask|work task\") ~ str_replace_all(column, \"\\\\b\\\\d+\\\\.\\\\s*\", \"\"),\n  TRUE ~ column)) %&gt;% \n  mutate(column = case_when(\n    str_detect(column, \"worktask|work task\") ~ str_replace_all(column, \n                                       setNames(rep(\"\", length(phrases_to_remove)), phrases_to_remove)),\n    TRUE ~ column),\n\n    value = case_when(\n    str_detect(column, \"worktask|work task\") ~ str_replace_all(value, \n                                       setNames(rep(\"\", length(phrases_to_remove)), \n                                    paste0(\"(?i)\", phrases_to_remove))),\n    TRUE ~ value)\n  ) %&gt;% \n  mutate(column = case_when(\n    column == \"components & take/samples:\" ~ \"components & take:\",\n    TRUE ~ column\n  )) %&gt;% \n  # Stuff after colon\n  # =====================\n  mutate(\n    # Extract the text after the colon (if present)\n    value_extracted = str_extract(column, \"(?&lt;=: ).*\"),\n    # Combine the extracted text with the value column\n    value = ifelse(is.na(value), value_extracted, paste(value_extracted, value, sep = \" \")),\n    # Remove the text after the colon in the column column\n    column = str_extract(column, \"^[^:]+\")\n  ) %&gt;%\n  select(-value_extracted) %&gt;% \n  mutate(value = str_remove_all(value, \"NA\"))\n\n  return(df_clean)\n\n\n}\n\n\nmake_df_shape = function(df){\n\ndf_wide = df %&gt;% \n  pivot_wider(\n    names_from = column,\n    values_from = value,\n    values_fn = list(value = ~ paste(unique(.), collapse = \", \"))\n  ) %&gt;% \n  janitor::clean_names()\n\nreturn(df_wide)\n\n\n}",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Analyze the jsonl"
    ]
  },
  {
    "objectID": "etl/4.transform_json_to_tabular.html#clean-each-json-file",
    "href": "etl/4.transform_json_to_tabular.html#clean-each-json-file",
    "title": "Analyze the jsonl",
    "section": "Clean each json file",
    "text": "Clean each json file\n\n# 89561_1\n# ============\n\ndf_8956_1 = foia_json_to_df(here(\"data/processed/montana-foia-parsed/8956_1_result.jsonl\"), \"8956_1\")\ndf_8956_1_clean = df_8956_1  %&gt;% \n  clean_parsed()\ndf_89561_1_final = df_8956_1_clean  %&gt;% \n  make_df_shape() %&gt;% \n   select(c(file, entry, worktask_for, work_date, agreement, property,activity,activity_measurements, conflict_loss, components_take, remarks, project, flagged))\n\n\n# 9430_2\n# ========\n\n\ndf_9430_2 = foia_json_to_df(here(\"data/processed/montana-foia-parsed/9430_2_result.jsonl\"), \"9430_2\")\ndf_9430_2_clean = df_9430_2  %&gt;% \n  clean_parsed()\ndf_9430_2_final = df_9430_2_clean  %&gt;% \n  make_df_shape() %&gt;% \n   select(c(file, entry, worktask_for, work_date, agreement, property,activity,activity_measurements, conflict_loss, components_take, remarks, project, flagged))\n\n\n# 9557_1\n# ==========\n\ndf_9557_1 = foia_json_to_df(here(\"data/processed/montana-foia-parsed/9557_1_result.jsonl\"), \"9557_1\")\ndf_9557_1_clean = df_9557_1  %&gt;% \n  clean_parsed()\ndf_9557_1_final = df_9557_1_clean  %&gt;% \n  make_df_shape() %&gt;% \n   select(c(file, entry, worktask_for, work_date, agreement, property,activity,activity_measurements, conflict_loss, components_take, remarks, project, flagged))\n\n\n\n# 9557_2 \n# ==========\n\ndf_9557_2 = foia_json_to_df(here(\"data/processed/montana-foia-parsed/9557_2_result.jsonl\"), \"9557_2\")\ndf_9557_2_clean = df_9557_2  %&gt;% \n  clean_parsed()\ndf_9557_2_final = df_9557_2_clean  %&gt;% \n  make_df_shape() %&gt;% \n   select(c(file, entry, worktask_for, work_date, agreement, property,activity,activity_measurements, conflict_loss, components_take, remarks, project, flagged))\n\n\n# 9557_3\n# ==========\n\ndf_9557_3 = foia_json_to_df(here(\"data/processed/montana-foia-parsed/9557_3_result.jsonl\"), \"9557_3\")\ndf_9557_3_clean = df_9557_3  %&gt;% \n  clean_parsed()\ndf_9557_3_final = df_9557_3_clean  %&gt;% \n  make_df_shape() %&gt;% \n   select(c(file, entry, worktask_for, work_date, agreement, property,activity,activity_measurements, conflict_loss, components_take, remarks, project, flagged))\n\n\n# 9557_4\n# ==========\ndf_9557_4 = foia_json_to_df(here(\"data/processed/montana-foia-parsed/9557_4_result.jsonl\"), \"9557_4\")\ndf_9557_4_clean = df_9557_4  %&gt;% \n  clean_parsed()\ndf_9557_4_final = df_9557_4_clean  %&gt;% \n  make_df_shape() %&gt;% \n   select(c(file, entry, worktask_for, work_date, agreement, property,activity,activity_measurements, conflict_loss, components_take, remarks, project, flagged))",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Analyze the jsonl"
    ]
  },
  {
    "objectID": "etl/4.transform_json_to_tabular.html#combine-files",
    "href": "etl/4.transform_json_to_tabular.html#combine-files",
    "title": "Analyze the jsonl",
    "section": "Combine files",
    "text": "Combine files\n\n# no_physical_harm_words = c(\"BARRIER\", \"FENCING\", \"RANGE RIDER\")\n# physical_harm_words = c(\"TRAP\", \"SRES\", \"FOOTHOLD\")\n# death_aim_words = c(\"CYANIDE\", \"FIREARM\", \"FIXED WING\")\n# action_maintenance = c(\"CHECKED\")\n\n\nextract_date_outside_parentheses &lt;- function(text) {\n  str_extract_all(text, \"(?&lt;!\\\\()\\\\b\\\\d{2}/\\\\d{2}/\\\\d{4}\\\\b(?!\\\\))\") %&gt;%\n    unlist() %&gt;%\n    .[!grepl(\"\\\\(\", .)]\n}\n\nwork_orders_df_raw = bind_rows(df_89561_1_final,df_9430_2_final,df_9557_1_final,df_9557_2_final, df_9557_3_final,df_9557_4_final)\n\nwork_orders_df_clean = work_orders_df_raw  %&gt;% \n       mutate(x_entry_date = gsub(\".*\\\\(Entry Date: (.*?)\\\\).*\", \"\\\\1\", work_date),\n         x_work_date = map_chr(work_date, ~ paste(extract_date_outside_parentheses(.x), collapse = \", \")), .after = work_date\n  ) %&gt;%\n  # mutate(x_work_date = str_remove(x_work_date, \"\\\\(Entry Date:\")) %&gt;% \n  mutate(\n    x_work_date = as.Date(x_work_date, format = \"%m/%d/%Y\"),\n    x_entry_date = as.Date(x_entry_date, format = \"%m/%d/%Y\"),\n    x_work_year = lubridate::year(x_work_date)\n  )\n\nextract_patterns &lt;- function(text) {\n  matches &lt;- str_extract_all(text, \"\\\\d+ EA \\\\w+ KILLED\")[[1]]\n  return(matches)\n}\n\n\nanimals_df = work_orders_df_clean  %&gt;% \n    mutate(across(where(is.character), str_squish)) %&gt;% \n    mutate(id = glue(\"{file}-{entry}\"), .before = file) %&gt;% \n     # Replace spelling errors\n  mutate(components_take = str_replace_all(components_take, \"Стр\", \"Cmp\")) %&gt;% \n  mutate(components_take = str_replace_all(components_take, \" BA \", \" EA \")) %&gt;% \n    mutate(x_threat_of = case_when(\n    str_detect(tolower(conflict_loss), \"damage threat of|damage threat\") ~ TRUE,\n    TRUE ~ FALSE\n  )) %&gt;% \n  mutate(x_predation = case_when(\n    str_detect(tolower(conflict_loss), \"predation\" ) ~ TRUE,\n    str_detect(tolower(conflict_loss), \"injury\") ~ TRUE,\n    TRUE ~ FALSE\n  )) %&gt;% \n  mutate(x_reason = case_when(\n\n    x_threat_of == TRUE & x_predation == TRUE ~ \"predation\",\n    x_threat_of == FALSE & x_predation == TRUE ~ \"predation\",\n    x_threat_of == TRUE & x_predation == FALSE ~ \"threat\",\n    TRUE ~ \"neither\"\n\n  )) %&gt;% \n  mutate(x_result = case_when(\n    str_detect(components_take, \"No Components/Take|No Components/ Take\") ~ \"no_components_take\",\n    str_detect(components_take, \"KILLED\") ~ \"killed\",\n    str_detect(components_take, \"SAMPLED\") ~ \"sampled\",\n    TRUE ~ NA_character_\n  )) %&gt;% \n  rowwise() %&gt;%\n  mutate(matches = list(extract_patterns(components_take))) %&gt;% \n  ungroup() \n \n\n\ncomponents_df = animals_df  %&gt;% \n    select(id, components_take)  %&gt;% \n    mutate(across(where(is.character), str_squish)) %&gt;% \n    separate_longer_delim(components_take, delim = \"Cmp\") %&gt;% \n    filter(components_take != \"\") %&gt;% \n    mutate(x_action = case_when(\n      str_detect(components_take, \"SET\") ~ \"set\",\n      str_detect(components_take, \"CHECK\") ~ \"check\",\n       str_detect(components_take, \"APPLIED|USED\") ~ \"apply\",\n      str_detect(components_take, \"REMOVE\") ~ \"remove\",\n      str_detect(components_take, \"FIRED\") ~ \"fire\",\n      str_detect(components_take, \"TESTED\") ~ \"tested\",\n      TRUE ~ NA_character_\n      \n    )) %&gt;% \n  \n  mutate(x_item = case_when(\n    str_detect(components_take, \"BARRIER\") ~ \"barrier\", \n    str_detect(components_take, \"FENCING\") ~ \"fencing\", \n    str_detect(components_take, \"RANGE RIDER\") ~ \"range_rider\",\n    str_detect(components_take, \"TRAP\") ~ \"trap\",\n    str_detect(components_take, \"SRES\") ~ \"snare\",\n    str_detect(components_take, \"CYANIDE\") ~ \"cyanide\",\n    str_detect(components_take, \"FIREARM\") ~ \"fire_arm\",\n    str_detect(components_take, \"FIXED WING\") ~ \"fixed_wing\",\n    str_detect(components_take, \"HELICOPTER\") ~ \"helicopter\",\n    str_detect(components_take, \"GAS\") ~ \"gas\",\n    TRUE ~ NA_character_\n\n\n  ))  %&gt;% \n   mutate(x_result = case_when(\n    str_detect(components_take, \"No Components/Take\") ~ \"no_components_take\",\n    str_detect(components_take, \"KILLED\") ~ \"killed\",\n    str_detect(components_take, \"TRANSFER\") ~ \"transfer\",\n    TRUE ~ NA_character_\n  ))  %&gt;% \n  # strict category is lethal if it not mean to dissuade, and intention is to kill aka cyanaide, fire arm, and gas\n  mutate(x_lethal_strict = case_when(\n    str_detect(x_item, \"cyanide|fire_arm|gas\") ~ 1,\n    TRUE ~ 0\n\n  )) %&gt;% \n  # lax  is lethal if the  items are not, like guns, cyanide, gas\n\n  mutate(x_lethal_lax = case_when(\n    str_detect(x_item, \"barrier|fencing|range_rider|helicopter|fixed_wing\") | is.na(x_item) == TRUE ~ 0,\n    TRUE ~ 1\n\n  ))  %&gt;% \n  filter(!is.na(x_item)) %&gt;% \n  mutate(x_item_count = case_when(\n    !is.na(x_item) ~ 1,\n    TRUE ~ 0 \n  ))\n\n\n# Create summaries\n# ==================\n\n\ncomponent_summary_strict_df = components_df  %&gt;% \n  summarize(lethal_strict_sum = sum(x_lethal_strict), x_items_actions = paste(glue(\"{x_item}:{x_action}\"), collapse = \", \"), x_number_items = sum(x_item_count), .by = id)\n\ncomponent_summary_lax_df = components_df  %&gt;% \n  summarize(lethal_lax_sum = sum(x_lethal_lax) , x_items_actions = paste(glue(\"{x_item}:{x_action}\"), collapse = \", \"), .by = id) %&gt;% \n  select(-x_items_actions)\n\ncombined_summary_df = full_join(component_summary_strict_df, component_summary_lax_df, join_by(id)) \n\n\n# Combine back\n# =============\n\nanimals_df = left_join(animals_df,combined_summary_df, join_by(id) )",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Analyze the jsonl"
    ]
  },
  {
    "objectID": "etl/4.transform_json_to_tabular.html#export-files",
    "href": "etl/4.transform_json_to_tabular.html#export-files",
    "title": "Analyze the jsonl",
    "section": "Export files",
    "text": "Export files\n\nsaveRDS(animals_df, here(\"data/processed/usda_workorders.RDS\"))",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Analyze the jsonl"
    ]
  },
  {
    "objectID": "etl/2.etl_download_pdr_g.html",
    "href": "etl/2.etl_download_pdr_g.html",
    "title": "Download PDR G Report",
    "section": "",
    "text": "Code\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Download PDR G Report"
    ]
  },
  {
    "objectID": "etl/2.etl_download_pdr_g.html#killed-euthenized-tables",
    "href": "etl/2.etl_download_pdr_g.html#killed-euthenized-tables",
    "title": "Download PDR G Report",
    "section": "Killed Euthenized tables",
    "text": "Killed Euthenized tables\nThe urls to the killed/euthized tables are the following:\n\n\nCode\nkilled_euthanized = list(\n  \"2023\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2023:KILLED_EUTH:0:k6Slc6m5armtqanTaYXJqp-Rrg==\",\n  \"2022\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2022:KILLED_EUTH:0:k6Slcqm5armtqanTaYXJqp-Rrg==\",\n  \"2021\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2021:KILLED_EUTH:0:k6Slcam5armtqanTaYXJqp-Rrg==\",\n  \"2020\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2020:KILLED_EUTH:0:k6SlcKm5armtqanTaYXJqp-Rrg==\",\n  \"2019\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2019:KILLED_EUTH:0:k6Skeam5armtqanTaYXJqp-Rrg==\",\n  \"2018\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2018:KILLED_EUTH:0:k6SkeKm5armtqanTaYXJqp-Rrg==\",\n  \"2017\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2017:KILLED_EUTH:0:k6Skd6m5armtqanTaYXJqp-Rrg==\",\n  \"2016\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2016:KILLED_EUTH:0:k6Skdqm5armtqanTaYXJqp-Rrg==\",\n  \"2015\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2015:KILLED_EUTH:0:k6Skdam5armtqanTaYXJqp-Rrg==\",\n  \"2014\" = \"https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2014:KILLED_EUTH:0:k6SkdKm5armtqanTaYXJqp-Rrg==\"\n)\n\nkilled_euthanized\n\n\nDownload all of the tables and combine into one dataframe\n\n\nCode\ndownload_killed_report = function(item, type){\n    \n   \n    url = item[1][[1]]\n\n    page &lt;- read_html(url)\n\n    table &lt;- page %&gt;% \n        html_nodes(\"table\") %&gt;%  \n        .[5] %&gt;% \n        html_table(fill = TRUE)\n\n    if (type == \"overall\"){\n        data_frame &lt;- as.data.frame(table) %&gt;% \n            janitor::row_to_names(row_number = 1) %&gt;% \n            janitor::clean_names() %&gt;% \n            mutate(across(where(is.character), tolower)) %&gt;% \n            filter(!str_detect(species, \"species total\"))\n\n    } else if ( type == \"state\"){\n\n        data_frame &lt;- as.data.frame(table) %&gt;% \n            janitor::row_to_names(row_number = 1) %&gt;% \n            janitor::clean_names() %&gt;% \n            mutate(across(where(is.character), tolower)) %&gt;% \n            filter(capture_restraint_dispersal_method   != \"totals\")\n\n    }\n\n    return(data_frame)\n\n}\n\n\nall_killed_euthanized = killed_euthanized  %&gt;%   \n    purrr::map(\\(x){download_killed_report(x, \"overall\")}, .progress = TRUE) %&gt;% \n    purrr::list_rbind(names_to = \"year\")\n\n\n\n\nCode\nglimpse(all_killed_euthanized)\nwrite_csv(all_killed_euthanized, here(\"data/processed/20240926--all_killed_euthanized_2014-2023.csv\"))",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Download PDR G Report"
    ]
  },
  {
    "objectID": "etl/2.etl_download_pdr_g.html#state-filtered",
    "href": "etl/2.etl_download_pdr_g.html#state-filtered",
    "title": "Download PDR G Report",
    "section": "State filtered",
    "text": "State filtered\nThis is the format of a url: https://www.aphis.usda.gov/pdr/PDR-G_Report?p=2015:STATE:AK:k6SkdanBda61qZ-1b2o=\nThe code below creates all the urls for each state for each year\n\n\nCode\n# Found by using inspect elements on webpage\n# ==========================================\n\nyear_codes = list(\"2023\" = \"k6Slc6nBda61qZ\",\n                  \"2022\" = \"k6SlcqnBda61qZ\",\n                  \"2021\" = \"k6SlcanBda61qZ\",\n                  \"2020\" = \"k6SlcKnBda61qZ\",\n                  \"2019\" = \"k6SkeanBda61qZ\",\n                  \"2018\" = \"k6SkeKnBda61qZ\",\n                  \"2017\" = \"k6Skd6nBda61qZ\",\n                  \"2016\" = \"k6SkdqnBda61qZ\",\n                  \"2015\" = \"k6SkdanBda61qZ\",\n                  \"2014\" = \"k6SkdKnBda61qZ\")\n\nstate_codes = list(\n'AK' = '-1b2o=',\n'AL' = '-1cGo=',\n'AR' = '-1dmo=',\n'AZ' = '-1fmo=',\n'CA' = '-3ZWo=',\n'CO' = '-3c2o=',\n'CT' = '-3eGo=',\n'DC' = '-4Z2o=',\n'DD' = '-4aGo=',\n'DE' = '-4aWo=',\n'FL' = '-6cGo=',\n'GA' = '-7ZWo=',\n'GU' = '-7eWo=',\n'HI' = '-8bWo=',\n'IA' = '-9ZWo=',\n'ID' = '-9aGo=',\n'IL' = '-9cGo=',\n'IN' = '-9cmo=',\n'KS' = '-_d2o=',\n'KY' = '-_fWo=',\n'LA' = '_AZWo=',\n'MA' = '_BZWo=',\n'MD' = '_BaGo=',\n'ME' = '_BaWo=',\n'MI' = '_BbWo=',\n'MN' = '_Bcmo=',\n'MO' = '_Bc2o=',\n'MS' = '_Bd2o=',\n'MT' = '_BeGo=',\n'NC' = '_CZ2o=',\n'ND' = '_CaGo=',\n'NE' = '_CaWo=',\n'NH' = '_CbGo=',\n'NJ' = '_Cbmo=',\n'NM' = '_CcWo=',\n'NV' = '_Cemo=',\n'NY' = '_CfWo=',\n'OH' = '_DbGo=',\n'OK' = '_Db2o=',\n'OR' = '_Ddmo=',\n'PA' = '_EZWo=',\n'PR' = '_Edmo=',\n'RI' = '_GbWo=',\n'SC' = '_HZ2o=',\n'SD' = '_HaGo=',\n'TN' = '_Icmo=',\n'TX' = '_IfGo=',\n'UT' = '_JeGo=',\n'VA' = '_KZWo=',\n#'None' = '_Jd4a-nA==',\n'VT' = '_KeGo=',\n'WA' = '_LZWo=',\n'WI' = '_LbWo=',\n'WV' = '_Lemo=',\n'WY' = '_LfWo=')\n\n# Create urls\n# ============\n\nyear_state_combos &lt;- expand.grid(year = names(year_codes), state = names(state_codes))\n\ncreate_url = function(year, state){\n    year_code = year_codes[[year]]\n    state_code = state_codes[[state]]\n    url = glue(\"https://www.aphis.usda.gov/pdr/PDR-G_Report?p={year}:STATE:{state}:{year_code}{state_code}\")\n    return(url)\n\n}\n\n\n\n\nstate_urls &lt;- setNames(\n  map2(year_state_combos$year, year_state_combos$state, ~ create_url(.x, .y)),\n  year_state_combos$year\n)\n\n\n\n\nCode\n# Download state tables\n\nall_states_df = state_urls %&gt;%   \n    purrr::map(\\(x){download_killed_report(x, \"state\")}, .progress = TRUE) %&gt;% \n    purrr::list_rbind(names_to = \"year\")\n\n\n\n\nCode\nglimpse(all_states_df)\nwrite_csv(all_states_df, here(\"data/processed/20240926--all_states_2014-2023.csv\"))\n\n\n\nPDR C\nhttps://www.aphis.usda.gov/pdr/PDR-C_Report?p=2023:STATE:TX:k6Slc6nBda61qZ_IfGo=\nhttps://www.aphis.usda.gov/pdr/PDR-C_Report?p=2023:INDEX:",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Download PDR G Report"
    ]
  },
  {
    "objectID": "presentation/index.html#scanned-pdfs",
    "href": "presentation/index.html#scanned-pdfs",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Scanned pdfs",
    "text": "Scanned pdfs",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#scanned-pdfs-1",
    "href": "presentation/index.html#scanned-pdfs-1",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Scanned pdfs",
    "text": "Scanned pdfs\n\n\n\nJust an image\nCan’t analyze for patterns (not in excel format)",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#google-pinpoint",
    "href": "presentation/index.html#google-pinpoint",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Google Pinpoint",
    "text": "Google Pinpoint\n\n\n\nPROS\n\nFree\nCan handle regular patterns\n\nCONS\n\nTake a long time to parse with big pdfs\nCan’t handle complex, custom patterns\nThe “deep linking” function which made fact checking hard",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#the-complex-custom-patterns",
    "href": "presentation/index.html#the-complex-custom-patterns",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "The complex, custom patterns",
    "text": "The complex, custom patterns\n\nRedaction boxes\nNew columns in certain work orders\nOne work order overlapping two pages",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#criteria",
    "href": "presentation/index.html#criteria",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Criteria",
    "text": "Criteria\n\nRun locally\n\nEasily reproducible by anyone with my code\nFree\n\nEasy to fact check each work order",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#roadmap",
    "href": "presentation/index.html#roadmap",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Roadmap",
    "text": "Roadmap\n\n\n\n\n\nflowchart TD\n  A(Separate each work order into its own screenshot) --&gt; B\n  B(Identify the cells of the scanned image) --&gt; C\n  C(OCR text and get positional data) --&gt; D\n  D(Do math to assign text to cell) --&gt; E\n  E(Use location to transform to csv)",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#screenshots",
    "href": "presentation/index.html#screenshots",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Screenshots",
    "text": "Screenshots\n\n\nimport cv2\nimport json\nimport os\nimport glob\n\nfrom table_parsing.image import convert_bounding_box\n\ndef make_screenshots(image_dir, json_dir, screenshot_output_path):\n\n\n    image_paths =  sorted(glob.glob(f\"{image_dir}/*jpg\"))\n    json_paths = sorted(glob.glob(f\"{json_dir}/*json\"))\n\n    first_half = None\n    counter = 0\n    for current_image_index in range(len(image_paths)):\n\n        image_path = image_paths[current_image_index]\n        json_path = json_paths[current_image_index]\n        \n        # print(\"current image: \" + str(current_image_index))\n        # Load the image\n        image = cv2.imread(image_path)\n\n        # Read the JSON file\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n\n        work_task_lines = []\n        for i, box in enumerate(data['observations']):\n            check_text = box[\"observation\"][\"text\"]\n            # print(check_text)\n            if \"for:\" in check_text and (\"WorkTask\" in check_text or \"Work Task\" in check_text):\n                # print(i)\n                x1, y1, x2, y2 =  convert_bounding_box(image,box[\"observation\"][\"bounds\"])\n                work_task_line = max(y1-30, 0)\n                work_task_lines.append(work_task_line)\n\n        work_task_lines = sorted(work_task_lines)\n\n        for line in work_task_lines:\n            cv2.line(image, (0, line), (image.shape[1], line), (0, 255, 0), 2)\n        # print(work_task_lines)\n        # # Display the image\n        # cv2.imshow('Image with Bounding Boxes', image)\n        # cv2.waitKey(0)\n        # cv2.destroyAllWindows()\n\n        # print(counter)\n\n        for i in range(len(work_task_lines)):\n            start_y = work_task_lines[i]\n            # print(\"star_y: \" + str(start_y))\n\n            if first_half is not None:\n                counter = counter +1\n                print(str(counter) + \"--\" + str(len(work_task_lines))) \n                # Crop from start to start y\n                second_half = image[:start_y, :]\n                combined_image = cv2.vconcat([first_half, second_half])\n\n                output_path = os.path.join(screenshot_output_path, f\"{counter:02}.jpg\")\n                cv2.imwrite(output_path, combined_image)\n\n                # cv2.imwrite(os.path.join(screenshot_output_path, f\"{counter:02}.5.jpg\"), second_half)\n\n                first_half = None\n\n            # If there is no next line\n            if i+1 &gt;= len(work_task_lines):\n                # Crop from start_y to the end of the image\n                first_half = image[start_y:, :]\n            else:\n                end_y = work_task_lines[i+1]\n                final_image = image[start_y:end_y, :]\n            \n            if first_half is None:\n                counter = counter + 1\n                # print(str(counter) + \"--\" + str(len(work_task_lines)))\n                output_path = os.path.join(screenshot_output_path, f\"{counter:02}.jpg\")\n                cv2.imwrite(output_path, final_image)\n\n            else:\n                pass\n\n    # Screenshot last image\n    counter = counter + 1\n    # print(str(counter) + \"--\" + str(len(work_task_lines)))\n    output_path = os.path.join(screenshot_output_path, f\"{counter:02}.jpg\")\n    cv2.imwrite(output_path, first_half)",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#screenshots-1",
    "href": "presentation/index.html#screenshots-1",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Screenshots",
    "text": "Screenshots\n\n\nDownload PDF file.",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#identify-cells",
    "href": "presentation/index.html#identify-cells",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Identify cells",
    "text": "Identify cells\n\n\nimport cv2\nimport json\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nimport glob\nfrom tqdm import tqdm\nimport os\nimport re\n\nfrom table_parsing.utils import stringify_keys\n\n# Handle input data\n# ==================================================\ndef process_image(image_path):\n\n    # Load the image\n    image = cv2.imread(image_path)\n\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    # Apply adaptive threshold to get binary image\n    binary = cv2.adaptiveThreshold(~gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, -10)\n    return image, binary\n\ndef load_data(json_path):\n    with open(json_path, 'r') as file:\n        data = json.load(file)\n\n    return(data)\n\n# Handle bounding boxes\n# ============================================\ndef draw_bounding_box(image, bounds, color=(0, 255, 0), thickness=2):\n    height, width, _ = image.shape\n    top_left = (int(bounds['x1'] * width), int((1 - bounds['y2']) * height))\n    bottom_right = (int(bounds['x2'] * width), int((1 - bounds['y1']) * height))\n    cv2.rectangle(image, top_left, bottom_right, color, thickness)\n\ndef convert_bounding_box(image, bounds):\n    height, width, _ = image.shape\n    # top left\n    x1 = int(bounds['x1'] * width)\n    y2 = int((1 - bounds['y2']) * height)\n    # Bottom right\n    x2 = int(bounds['x2'] * width)\n    y1 = int((1 - bounds['y1']) * height)\n\n    # Calculations\n    # width = x2 - x1\n    # height = y2 - y1\n\n    return x1, y1, x2, y2\n\ndef draw_all_word_bounding_boxes(image,data):\n    # Loop through the observations and subBounds to draw bounding boxes\n    for item in data['observations']:\n        # Draw bounding box for the entire phrase\n        draw_bounding_box(image, item[\"observation\"]['bounds'], color=(255, 0, 0))\n        # print(item[\"observation\"]['text'])\n    return(image)\n\n        # Draw bounding boxes for each word\n        # for subBound in item[\"observation\"]['subBounds']:\n        #     draw_bounding_box(image, subBound['bounds'], color=(0, 255, 0))\n\n    # # Display the image\n    # cv2.imshow('Image with Bounding Boxes', image)\n    # cv2.waitKey(0)\n    # cv2.destroyAllWindows()\n\n# Cluster points\n# ======================================\n\ndef find_horizontal_vertical_lines(binary, horizontal_scale_factor, vertical_scale_factor):\n    # # Parameters to adjust\n    # horizontal_scale_factor = 15  # Increase for larger structuring elements\n    # vertical_scale_factor = 50    # Increase for larger structuring elements\n    # dilation_size = 6            # Increase for more forgiving touch point detection\n\n    # Detect horizontal lines\n    horizontal = binary.copy()\n    cols = horizontal.shape[1]\n    horizontal_size = cols // horizontal_scale_factor\n    horizontal_structure = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))\n    horizontal = cv2.erode(horizontal, horizontal_structure)\n    horizontal = cv2.dilate(horizontal, horizontal_structure)\n\n    # Detect vertical lines\n    vertical = binary.copy()\n    rows = vertical.shape[0]\n    vertical_size = rows // vertical_scale_factor\n    vertical_structure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vertical_size))\n    vertical = cv2.erode(vertical, vertical_structure)\n    vertical = cv2.dilate(vertical, vertical_structure)\n\n    # Combine horizontal and vertical lines\n    # grid = cv2.add(horizontal, vertical)\n\n    return(horizontal, vertical)\n\n\n\n\n\n# Find touches (where lines touch but don't intersect)\ndef find_touches(image, horizontal, vertical, dilation_size):\n\n    # get touches\n    # ==============\n    touch_points = []\n\n    # Dilate horizontal and vertical lines to ensure touching points are detected\n    horizontal_dilated = cv2.dilate(horizontal, np.ones((dilation_size, dilation_size), np.uint8))\n    vertical_dilated = cv2.dilate(vertical, np.ones((dilation_size, dilation_size), np.uint8))\n    \n    # Combine dilated lines to find touch points\n    touch_points_img = cv2.bitwise_and(horizontal_dilated, vertical_dilated)\n    \n    # Get touch points coordinates\n    touch_points_coords = np.argwhere(touch_points_img == 255)\n    for point in touch_points_coords:\n        touch_points.append((point[1], point[0]))\n\n    # cluster touch points\n    # ======================\n    dbscan = DBSCAN(eps=10, min_samples=1).fit(touch_points)\n    clustered_touch_points = []\n    for label in np.unique(dbscan.labels_):\n        cluster = np.array(touch_points)[dbscan.labels_ == label]\n        if cluster.size &gt; 0:\n            clustered_touch_points.append(np.mean(cluster, axis=0).astype(int))\n\n\n    \n    return(clustered_touch_points)\n\n\n\ndef draw_clustered_touch_points(image, clustered_touch_points):\n    # Draw clustered touch points for visualization\n    for point in clustered_touch_points:\n        cv2.circle(image, tuple(point), 5, (255, 0, 0), -1)\n\n\n    # Display the image with detected lines, cells, intersections, and touches\n    return(image)\n\n\n# Final grid lines\n# ========================================\n\n# Identify and draw the outermost vertical and horizontal lines\ndef make_final_lines(image, clustered_touch_points):\n    if not clustered_touch_points:\n        return image\n\n    # touch_points_np = np.array(touch_points)\n    clustered_touch_points_np = np.array(clustered_touch_points)\n\n    # Identify the outermost vertical lines\n    left_most = np.min(clustered_touch_points_np[:, 0])\n    right_most = np.max(clustered_touch_points_np[:, 0])\n    middle = np.median(clustered_touch_points_np[:, 0])\n\n    # Identify the outermost horizontal lines\n    top_most = np.min(clustered_touch_points_np[:, 1])\n    bottom_most = np.max(clustered_touch_points_np[:, 1])\n\n\n\n    # Draw vertical lines\n    cv2.line(image, (left_most, 0), (left_most, image.shape[0]), (0, 255, 0), 2)\n    cv2.line(image, (int(middle), 0), (int(middle), image.shape[0]), (0, 255, 0), 2)\n    cv2.line(image, (right_most, 0), (right_most, image.shape[0]), (0, 255, 0), 2)\n\n    # Draw horizontal lines\n    cv2.line(image, (0, top_most), (image.shape[1], top_most), (0, 255, 0), 2)\n    cv2.line(image, (0, bottom_most), (image.shape[1], bottom_most), (0, 255, 0), 2)\n\n   \n\n    # Remove touch points that are within 5 pixels of top_most and bottom_most y-coordinates\n    filtered_touch_points = [point for point in clustered_touch_points if not (top_most - 5 &lt;= point[1] &lt;= top_most + 5 or bottom_most - 5 &lt;= point[1] &lt;= bottom_most + 5)]\n    filtered_touch_points_np = np.array(filtered_touch_points)\n\n    # Identify unique y-coordinates of filtered touch points\n    unique_y_coords = np.array(sorted(set(filtered_touch_points_np[:, 1]))).reshape(-1, 1)\n\n    # Cluster the y-coordinates using DBSCAN\n    dbscan = DBSCAN(eps=10, min_samples=1).fit(unique_y_coords)\n    clustered_y_coords = []\n\n    for label in np.unique(dbscan.labels_):\n        cluster = unique_y_coords[dbscan.labels_ == label]\n        if cluster.size &gt; 0:\n            clustered_y_coords.append(np.mean(cluster))\n\n    clustered_y_coords = sorted(clustered_y_coords)\n\n\n    \n    for y in clustered_y_coords:\n        y = int(y)\n\n        cv2.line(image, (left_most, y), (right_most, y), (0, 255, 0), 2)\n\n\n\n    return image, [top_most] + clustered_y_coords + [bottom_most], [left_most, middle, right_most]\n\n\n# Assigning data \n# ============================================\n\n# Function to calculate the intersection area of two rectangles\ndef intersection_area(box1, box2):\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n    return intersection\n\n\ndef assign_data(image, data, horizontal_lines, vertical_lines):\n\n    row_result = {}\n\n    for item in data['observations']:\n        \n        x1, y1, x2, y2 = convert_bounding_box(image, item[\"observation\"]['bounds'] )\n        text = item[\"observation\"]['text']\n\n\n        bounding_box = (x1, y1, x2, y2)\n\n\n        # Determine the cell containing the majority of the bounding box\n        max_intersection = 0\n        cell_with_max_intersection = (0, 0)\n\n        for i in range(len(horizontal_lines) - 1):\n            for j in range(len(vertical_lines) - 1):\n                # Define the bounding box for the current cell\n                cell_box = (vertical_lines[j], horizontal_lines[i], vertical_lines[j + 1], horizontal_lines[i + 1])\n                # print(\"bounding_box: \" + str(bounding_box))\n                # print(\"cell box: \" + str(cell_box))\n\n                \n                # Calculate the intersection area between the bounding box and the cell\n                intersection = intersection_area(bounding_box, cell_box)\n                # print(\"intersection: \" + str(intersection))\n                # Update the cell with the maximum intersection area\n                if intersection &gt; max_intersection:\n                    max_intersection = intersection\n                    cell_with_max_intersection = (i, j)\n\n        cell_key = cell_with_max_intersection\n        if cell_key in row_result:\n            row_result[cell_key] += \" \" + text\n        else:\n            row_result[cell_key] = text\n\n\n    return(row_result)\n\ndef natural_sort_key(s):\n    return [int(text) if text.isdigit() else text for text in re.split(r'(\\d+)', s)]\n\ndef make_result(image_dir, json_dir, jsonl_output_path, annotated_output_path):\n\n    image_paths =  sorted(glob.glob(f\"{image_dir}/*jpg\") , key=natural_sort_key)\n\n    json_paths = sorted(glob.glob(f\"{json_dir}/*json\"), key=natural_sort_key)\n\n\n    with open(jsonl_output_path, 'w') as jsonl_file:\n        for i in range(len(image_paths)):\n            image_path = image_paths[i]\n            json_path = json_paths[i]\n\n            file_name = os.path.basename(image_path)\n\n            image, binary = process_image(image_path)\n            data = load_data(json_path)\n\n            # image = draw_all_word_bounding_boxes(image, data)\n\n            horizontal_lines, vertical_lines = find_horizontal_vertical_lines(binary, horizontal_scale_factor = 15, vertical_scale_factor = 30)\n\n            clustered_touch_points = find_touches(image, horizontal_lines, vertical_lines, dilation_size=6)\n\n            image = draw_clustered_touch_points(image, clustered_touch_points)\n\n            image_with_lines, horizontal_lines, vertical_lines = make_final_lines(image, clustered_touch_points)\n\n            result = assign_data(image, data, horizontal_lines, vertical_lines)\n\n        \n\n            # Save results\n            # ============\n            cv2.imwrite(os.path.join(annotated_output_path, file_name), image_with_lines)\n            # Write the result to the JSONL file\n            jsonl_file.write(json.dumps(stringify_keys(result)) + '\\n')",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#identify-cells-1",
    "href": "presentation/index.html#identify-cells-1",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Identify cells",
    "text": "Identify cells",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#identify-cells-2",
    "href": "presentation/index.html#identify-cells-2",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Identify cells",
    "text": "Identify cells",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#apples-live-text-feature",
    "href": "presentation/index.html#apples-live-text-feature",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Apple’s live text feature",
    "text": "Apple’s live text feature",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#get-positional-data-of-text",
    "href": "presentation/index.html#get-positional-data-of-text",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Get positional data of text",
    "text": "Get positional data of text\n\n{\n  \"info\" : {\n    \"program\" : \"textra\",\n    \"version\" : \"0.2.1\"\n  },\n  \"observations\" : [\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"x2\" : 0.16144200584639501,\n          \"y2\" : 0.95008912650623889,\n          \"x1\" : 0.11912225663009407,\n          \"y1\" : 0.98039215680926917\n        },\n        \"text\" : \"36.\",\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              3\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.9803921565062389,\n              \"x2\" : 0.1614420054231975,\n              \"x1\" : 0.11912225705329153,\n              \"y2\" : 0.95008912680926916\n            },\n            \"text\" : \"36.\"\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 0.5,\n        \"subBounds\" : [\n          {\n            \"text\" : \"Work\",\n            \"offset\" : [\n              0,\n              4\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.23667711598746083,\n              \"y2\" : 0.946524064171123,\n              \"x1\" : 0.18652037617554859,\n              \"y1\" : 0.97504456292335118\n            }\n          },\n          {\n            \"text\" : \"Task\",\n            \"bounds\" : {\n              \"y2\" : 0.946524064171123,\n              \"y1\" : 0.97504456327985745,\n              \"x1\" : 0.23981191222570533,\n              \"x2\" : 0.28056426332288403\n            },\n            \"offset\" : [\n              5,\n              9\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"x2\" : 0.31504702194357365,\n              \"x1\" : 0.28369905956112851,\n              \"y1\" : 0.97504456327985745,\n              \"y2\" : 0.946524064171123\n            },\n            \"text\" : \"for:\",\n            \"offset\" : [\n              10,\n              14\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"y2\" : 0.946524064171123,\n              \"y1\" : 0.97504456327985745,\n              \"x1\" : 0.31818181818181818,\n              \"x2\" : 0.34952978056426331\n            },\n            \"offset\" : [\n              15,\n              18\n            ],\n            \"text\" : \"(b)\"\n          },\n          {\n            \"text\" : \"(6)\",\n            \"offset\" : [\n              19,\n              22\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.35266457680250785,\n              \"y1\" : 0.97504456327985745,\n              \"y2\" : 0.94652406438502679,\n              \"x2\" : 0.38871472949843267\n            }\n          }\n        ],\n        \"bounds\" : {\n          \"x1\" : 0.18652037415360501,\n          \"y1\" : 0.97504456320855615,\n          \"x2\" : 0.38871473152037617,\n          \"y2\" : 0.9465240640998217\n        },\n        \"text\" : \"Work Task for: (b) (6)\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"x1\" : 0.44200626929467085,\n          \"y2\" : 0.95008912675579327,\n          \"y1\" : 0.9696969698930481,\n          \"x2\" : 0.49686520346394986\n        },\n        \"text\" : \"Direct\",\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"text\" : \"Direct\",\n            \"bounds\" : {\n              \"x1\" : 0.44200626984326019,\n              \"y1\" : 0.96969696969696972,\n              \"y2\" : 0.95008912695187164,\n              \"x2\" : 0.49686520291536052\n            },\n            \"offset\" : [\n              0,\n              6\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"y2\" : 0.91257735462201994,\n          \"y1\" : 0.9305599004304822,\n          \"x1\" : 0.18810392744810853,\n          \"x2\" : 0.25546974122659416\n        },\n        \"text\" : \"Control\",\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"text\" : \"Control\",\n            \"bounds\" : {\n              \"x1\" : 0.18810392586520183,\n              \"y2\" : 0.91257735442674492,\n              \"y1\" : 0.93055990062575722,\n              \"x2\" : 0.25546974280950085\n            },\n            \"offset\" : [\n              0,\n              7\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"Edit Sampling Disposition\",\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"x1\" : 0.29780564742946702,\n              \"x2\" : 0.34247648902821315,\n              \"y1\" : 0.93048128327985735,\n              \"y2\" : 0.90909090909090906\n            },\n            \"offset\" : [\n              0,\n              4\n            ],\n            \"text\" : \"Edit\"\n          },\n          {\n            \"offset\" : [\n              5,\n              13\n            ],\n            \"text\" : \"Sampling\",\n            \"bounds\" : {\n              \"x2\" : 0.43652037617554856,\n              \"x1\" : 0.34482758620689657,\n              \"y2\" : 0.90909090909090906,\n              \"y1\" : 0.93048128342245984\n            }\n          },\n          {\n            \"offset\" : [\n              14,\n              25\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.53761755485893414,\n              \"y2\" : 0.90909090937611414,\n              \"x1\" : 0.43887147335423199,\n              \"y1\" : 0.93048128342245984\n            },\n            \"text\" : \"Disposition\"\n          }\n        ],\n        \"bounds\" : {\n          \"x2\" : 0.53761755725705318,\n          \"y2\" : 0.90909090916221036,\n          \"x1\" : 0.29780564503134793,\n          \"y1\" : 0.93048128349376114\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"y1\" : 0.90552584623885923,\n              \"y2\" : 0.88235294117647056,\n              \"x1\" : 0.18808777733542315,\n              \"x2\" : 0.31798588388765869\n            },\n            \"offset\" : [\n              0,\n              14\n            ],\n            \"text\" : \"Entered\\/Edited\"\n          },\n          {\n            \"text\" : \"by\",\n            \"bounds\" : {\n              \"x2\" : 0.34345610145491118,\n              \"x1\" : 0.32053290564438391,\n              \"y1\" : 0.90552584670231728,\n              \"y2\" : 0.88235294117647056\n            },\n            \"offset\" : [\n              15,\n              17\n            ]\n          },\n          {\n            \"offset\" : [\n              18,\n              22\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.34600312321163645,\n              \"x2\" : 0.38930249307596571,\n              \"y2\" : 0.88235294117647056,\n              \"y1\" : 0.90552584670231728\n            },\n            \"text\" : \"John\"\n          },\n          {\n            \"bounds\" : {\n              \"x2\" : 0.45768024843260191,\n              \"y2\" : 0.88235294117647056,\n              \"x1\" : 0.39184951483269098,\n              \"y1\" : 0.90552584670231728\n            },\n            \"text\" : \"Steuber\",\n            \"offset\" : [\n              23,\n              30\n            ]\n          }\n        ],\n        \"text\" : \"Entered\\/Edited by John Steuber\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y1\" : 0.9055258464705882,\n          \"x1\" : 0.18808777463949844,\n          \"x2\" : 0.45768025112852667,\n          \"y2\" : 0.88235294094474159\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"y2\" : 0.90730837834224598,\n              \"x2\" : 0.61128526624869384,\n              \"y1\" : 0.93404634581105173,\n              \"x1\" : 0.56112852727272722\n            },\n            \"text\" : \"Flag?\",\n            \"offset\" : [\n              0,\n              5\n            ]\n          }\n        ],\n        \"bounds\" : {\n          \"x2\" : 0.6112852666666666,\n          \"y2\" : 0.90730837811942955,\n          \"y1\" : 0.93404634603386805,\n          \"x1\" : 0.56112852685475434\n        },\n        \"confidence\" : 1,\n        \"text\" : \"Flag?\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x1\" : 0.18815321844609226,\n          \"y2\" : 0.84093111714321089,\n          \"x2\" : 0.28833580851217094,\n          \"y1\" : 0.86138617326400346\n        },\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              4\n            ],\n            \"text\" : \"Work\",\n            \"bounds\" : {\n              \"y1\" : 0.86138617345490087,\n              \"x1\" : 0.18815321967438162,\n              \"x2\" : 0.23759269482473663,\n              \"y2\" : 0.84135996157891957\n            }\n          },\n          {\n            \"bounds\" : {\n              \"x2\" : 0.28833580728388153,\n              \"y1\" : 0.86094905011880474,\n              \"x1\" : 0.23987593017825068,\n              \"y2\" : 0.8409311169523136\n            },\n            \"text\" : \"Date:\",\n            \"offset\" : [\n              5,\n              10\n            ]\n          }\n        ],\n        \"text\" : \"Work Date:\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              10\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.79251246073001735,\n              \"x1\" : 0.18664027946636474,\n              \"y1\" : 0.81533067647150159,\n              \"x2\" : 0.28828134807988426\n            },\n            \"text\" : \"Agreement:\"\n          }\n        ],\n        \"text\" : \"Agreement:\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y2\" : 0.7925124601617406,\n          \"x1\" : 0.1866402807620976,\n          \"y1\" : 0.81533067703977824,\n          \"x2\" : 0.2882813467841514\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              10\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.51567398056426339,\n              \"y2\" : 0.84135472370766484,\n              \"x1\" : 0.42476489147335422,\n              \"y1\" : 0.86096256645276292\n            },\n            \"text\" : \"06\\/18\\/2019\"\n          }\n        ],\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x1\" : 0.42476489056426336,\n          \"x2\" : 0.5156739814733543,\n          \"y1\" : 0.86096256664884141,\n          \"y2\" : 0.84135472351158647\n        },\n        \"text\" : \"06\\/18\\/2019\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x2\" : 0.75391849447492165,\n          \"y2\" : 0.83778966112596553,\n          \"x1\" : 0.55172413710815049,\n          \"y1\" : 0.86096256665181226\n        },\n        \"text\" : \"(Entry Date: 07\\/22\\/2019)\",\n        \"subBounds\" : [\n          {\n            \"text\" : \"(Entry\",\n            \"offset\" : [\n              0,\n              6\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.60521159482226472,\n              \"y2\" : 0.83778966131907306,\n              \"x1\" : 0.55172413879310345,\n              \"y1\" : 0.86096256645870473\n            }\n          },\n          {\n            \"offset\" : [\n              7,\n              12\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.86096256684491979,\n              \"x2\" : 0.65360500820004452,\n              \"x1\" : 0.60775861657899,\n              \"y2\" : 0.83778966131907306\n            },\n            \"text\" : \"Date:\"\n          },\n          {\n            \"text\" : \"07\\/22\\/2019)\",\n            \"offset\" : [\n              13,\n              24\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.6561520299567698,\n              \"y1\" : 0.86096256684491979,\n              \"x2\" : 0.7539184927899687,\n              \"y2\" : 0.83778966131907306\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              3\n            ],\n            \"text\" : \"(b)\",\n            \"bounds\" : {\n              \"y1\" : 0.82709447350713017,\n              \"y2\" : 0.77540106951871657,\n              \"x1\" : 0.39655172568573666,\n              \"x2\" : 0.4476880877742947\n            }\n          },\n          {\n            \"text\" : \"(6),\",\n            \"offset\" : [\n              4,\n              8\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.52155172413793105,\n              \"y2\" : 0.77540106951871657,\n              \"x1\" : 0.45336990595611287,\n              \"y1\" : 0.82709447415329773\n            }\n          },\n          {\n            \"text\" : \"(b)\",\n            \"offset\" : [\n              9,\n              12\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.58405172413793105,\n              \"y1\" : 0.82709447415329773,\n              \"x1\" : 0.52723354231974917,\n              \"y2\" : 0.77540106951871657\n            }\n          },\n          {\n            \"offset\" : [\n              13,\n              16\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.64655172413793105,\n              \"y1\" : 0.82709447415329773,\n              \"y2\" : 0.77540106951871657,\n              \"x1\" : 0.58973354231974917\n            },\n            \"text\" : \"(3)\"\n          },\n          {\n            \"text\" : \"(A)\",\n            \"offset\" : [\n              17,\n              20\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.77540106951871657,\n              \"x2\" : 0.71003134559169279,\n              \"x1\" : 0.65223354231974917,\n              \"y1\" : 0.82709447415329773\n            }\n          }\n        ],\n        \"confidence\" : 0.5,\n        \"text\" : \"(b) (6), (b) (3) (A)\",\n        \"bounds\" : {\n          \"x2\" : 0.71003134755094044,\n          \"x1\" : 0.396551723726489,\n          \"y2\" : 0.77540106919563279,\n          \"y1\" : 0.82709447383021395\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"y2\" : 0.74674913099502405,\n          \"x1\" : 0.18811629417125428,\n          \"y1\" : 0.77018491574121795,\n          \"x2\" : 0.27113135340022909\n        },\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"text\" : \"Property:\",\n            \"bounds\" : {\n              \"x1\" : 0.18811629376148015,\n              \"y2\" : 0.74674913140219401,\n              \"y1\" : 0.77018491533404809,\n              \"x2\" : 0.27113135381000325\n            },\n            \"offset\" : [\n              0,\n              9\n            ]\n          }\n        ],\n        \"text\" : \"Property:\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"y2\" : 0.70231729026292333,\n          \"y1\" : 0.72549019578877005,\n          \"x2\" : 0.38401253832288407,\n          \"x1\" : 0.18652037531347965\n        },\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              9\n            ],\n            \"text\" : \"Activity:\",\n            \"bounds\" : {\n              \"x1\" : 0.18652037778213162,\n              \"x2\" : 0.26293102887730613,\n              \"y2\" : 0.70231729055258474,\n              \"y1\" : 0.72549019549910865\n            }\n          },\n          {\n            \"text\" : \"232167123126\",\n            \"offset\" : [\n              10,\n              22\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.2654780506340314,\n              \"y1\" : 0.72549019607843135,\n              \"x2\" : 0.38401253585423206,\n              \"y2\" : 0.70231729055258474\n            }\n          }\n        ],\n        \"text\" : \"Activity: 232167123126\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"Activity\",\n        \"bounds\" : {\n          \"x1\" : 0.18808777498432602,\n          \"x2\" : 0.25705329222570533,\n          \"y1\" : 0.67914438516934039,\n          \"y2\" : 0.65775401083778973\n        },\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"y2\" : 0.6577540110516934,\n              \"y1\" : 0.67914438495543672,\n              \"x1\" : 0.18808777567398116,\n              \"x2\" : 0.25705329153605017\n            },\n            \"text\" : \"Activity\",\n            \"offset\" : [\n              0,\n              8\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"Measurements:\",\n        \"subBounds\" : [\n          {\n            \"text\" : \"Measurements:\",\n            \"bounds\" : {\n              \"x2\" : 0.31818181603970747,\n              \"y2\" : 0.63458110549613789,\n              \"x1\" : 0.18652037622779519,\n              \"y1\" : 0.65418894830659535\n            },\n            \"offset\" : [\n              0,\n              13\n            ]\n          }\n        ],\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x1\" : 0.18652037513061656,\n          \"x2\" : 0.31818181713688615,\n          \"y1\" : 0.65418894846999409,\n          \"y2\" : 0.63458110533273915\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"Conflict\",\n        \"bounds\" : {\n          \"x1\" : 0.18804212707963844,\n          \"x2\" : 0.26023373619021656,\n          \"y1\" : 0.59872947198931992,\n          \"y2\" : 0.57774111643092119\n        },\n        \"subBounds\" : [\n          {\n            \"text\" : \"Conflict\",\n            \"offset\" : [\n              0,\n              8\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.18804212830892908,\n              \"y1\" : 0.59872947233735208,\n              \"x2\" : 0.26023373496092594,\n              \"y2\" : 0.57774111608288903\n            }\n          }\n        ],\n        \"confidence\" : 1\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"& Loss:\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y1\" : 0.57230287497679577,\n          \"x1\" : 0.18811501012243048,\n          \"x2\" : 0.25545865919438998,\n          \"y2\" : 0.55069177603962349\n        },\n        \"subBounds\" : [\n          {\n            \"text\" : \"&\",\n            \"offset\" : [\n              0,\n              1\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.18811500922711974,\n              \"y1\" : 0.57230287532172841,\n              \"y2\" : 0.55085867016349144,\n              \"x2\" : 0.20451843678489462\n            }\n          },\n          {\n            \"offset\" : [\n              2,\n              7\n            ],\n            \"text\" : \"Loss:\",\n            \"bounds\" : {\n              \"y1\" : 0.57224125242535107,\n              \"x1\" : 0.20692370952325456,\n              \"x2\" : 0.25545866008970075,\n              \"y2\" : 0.55069177654466794\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"Components\",\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"text\" : \"Components\",\n            \"offset\" : [\n              0,\n              10\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.43950755706575395,\n              \"x1\" : 0.18821297698626274,\n              \"y1\" : 0.46245322762887275,\n              \"x2\" : 0.29454564203566597\n            }\n          }\n        ],\n        \"bounds\" : {\n          \"x2\" : 0.29454564217437784,\n          \"x1\" : 0.18821297684755084,\n          \"y1\" : 0.46245322782008669,\n          \"y2\" : 0.43950755687454002\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              1\n            ],\n            \"text\" : \"&\",\n            \"bounds\" : {\n              \"y2\" : 0.41354723707664887,\n              \"x1\" : 0.18808777622779516,\n              \"y1\" : 0.43672014221628053,\n              \"x2\" : 0.2059169265917476\n            }\n          },\n          {\n            \"offset\" : [\n              2,\n              15\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.43672014260249559,\n              \"x2\" : 0.33385579887669803,\n              \"y2\" : 0.41354723707664887,\n              \"x1\" : 0.20846394834847287\n            },\n            \"text\" : \"Take\\/Samples:\"\n          }\n        ],\n        \"confidence\" : 1,\n        \"text\" : \"& Take\\/Samples:\",\n        \"bounds\" : {\n          \"x1\" : 0.18808777501306162,\n          \"y1\" : 0.43672014240938806,\n          \"y2\" : 0.41354723688354134,\n          \"x2\" : 0.33385580009143151\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              8\n            ],\n            \"text\" : \"Remarks:\",\n            \"bounds\" : {\n              \"y2\" : 0.30783199789780724,\n              \"x2\" : 0.27106815845026078,\n              \"y1\" : 0.32674910715079897,\n              \"x1\" : 0.18661209063933379\n            }\n          }\n        ],\n        \"text\" : \"Remarks:\",\n        \"bounds\" : {\n          \"x2\" : 0.27106815660011696,\n          \"x1\" : 0.1866120924894776,\n          \"y1\" : 0.3267491071440769,\n          \"y2\" : 0.30783199790452931\n        },\n        \"confidence\" : 1\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y2\" : 0.21595561891730042,\n          \"x1\" : 0.18802405371508266,\n          \"y1\" : 0.23680730495637337,\n          \"x2\" : 0.25711701054505059\n        },\n        \"text\" : \"Project:\",\n        \"subBounds\" : [\n          {\n            \"text\" : \"Project:\",\n            \"offset\" : [\n              0,\n              8\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.23680730463596333,\n              \"x2\" : 0.25711700959380346,\n              \"y2\" : 0.21595561923771056,\n              \"x1\" : 0.18802405466632979\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 0.5,\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"x1\" : 0.42476489028213166,\n              \"y1\" : 0.72727272704354462,\n              \"x2\" : 0.48334639068681245,\n              \"y2\" : 0.70409982174688057\n            },\n            \"offset\" : [\n              0,\n              5\n            ],\n            \"text\" : \"FIELD\"\n          },\n          {\n            \"text\" : \"WRK\",\n            \"offset\" : [\n              6,\n              9\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.48589341244353768,\n              \"x2\" : 0.5317398040645922,\n              \"y1\" : 0.72727272727272729,\n              \"y2\" : 0.70409982174688057\n            }\n          },\n          {\n            \"text\" : \"(PERFORMED)\",\n            \"bounds\" : {\n              \"x1\" : 0.53428682582131748,\n              \"x2\" : 0.67084639146887592,\n              \"y2\" : 0.70409982184873954,\n              \"y1\" : 0.72727272727272729\n            },\n            \"offset\" : [\n              10,\n              21\n            ]\n          }\n        ],\n        \"text\" : \"FIELD WRK (PERFORMED)\",\n        \"bounds\" : {\n          \"x2\" : 0.670846393226601,\n          \"y1\" : 0.72727272720906555,\n          \"y2\" : 0.70409982168321883,\n          \"x1\" : 0.42476488852440664\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"6 HOURS\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y2\" : 0.64884135481283423,\n          \"x1\" : 0.42476489075235113,\n          \"y1\" : 0.66844919795008917,\n          \"x2\" : 0.50940438918495301\n        },\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              1\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.4247648912225705,\n              \"y1\" : 0.66844919784115664,\n              \"x2\" : 0.43769592476489028,\n              \"y2\" : 0.64884135472370774\n            },\n            \"text\" : \"6\"\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.66844919786096257,\n              \"x2\" : 0.50940438871473359,\n              \"x1\" : 0.43985109717868337,\n              \"y2\" : 0.64884135492176664\n            },\n            \"text\" : \"HOURS\",\n            \"offset\" : [\n              2,\n              7\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              6\n            ],\n            \"text\" : \"BEARS.\",\n            \"bounds\" : {\n              \"x1\" : 0.42633228840125392,\n              \"y1\" : 0.60962566822001529,\n              \"y2\" : 0.58823529411764708,\n              \"x2\" : 0.4945141065830721\n            }\n          },\n          {\n            \"offset\" : [\n              7,\n              12\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.60962566844919786,\n              \"x2\" : 0.5650470219435737,\n              \"y2\" : 0.58823529411764708,\n              \"x1\" : 0.49686520376175547\n            },\n            \"text\" : \"BLACK\"\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.60962566844919786,\n              \"x1\" : 0.56739811912225702,\n              \"x2\" : 0.63087774294670851,\n              \"y2\" : 0.58823529411764708\n            },\n            \"text\" : \"damage\",\n            \"offset\" : [\n              13,\n              19\n            ]\n          },\n          {\n            \"text\" : \"threat\",\n            \"offset\" : [\n              20,\n              26\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.60962566844919786,\n              \"x2\" : 0.68260188087774298,\n              \"y2\" : 0.58823529411764708,\n              \"x1\" : 0.63322884012539182\n            }\n          },\n          {\n            \"offset\" : [\n              27,\n              29\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.58823529411764708,\n              \"y1\" : 0.60962566844919786,\n              \"x2\" : 0.70376175548589337,\n              \"x1\" : 0.6849529780564263\n            },\n            \"text\" : \"of\"\n          },\n          {\n            \"text\" : \"EGGS\",\n            \"offset\" : [\n              30,\n              34\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.76175548110165703,\n              \"y2\" : 0.58823529419404119,\n              \"y1\" : 0.60962566844919786,\n              \"x1\" : 0.7061128526645768\n            }\n          }\n        ],\n        \"bounds\" : {\n          \"y2\" : 0.58823529404125285,\n          \"x2\" : 0.76175548349753708,\n          \"x1\" : 0.42633228600537404,\n          \"y1\" : 0.60962566837280363\n        },\n        \"text\" : \"BEARS. BLACK damage threat of EGGS\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"text\" : \"BEARS,\",\n            \"bounds\" : {\n              \"x2\" : 0.4945141065830721,\n              \"x1\" : 0.42633229334975364,\n              \"y2\" : 0.56149732620320858,\n              \"y1\" : 0.58288770040743576\n            },\n            \"offset\" : [\n              0,\n              6\n            ]\n          },\n          {\n            \"offset\" : [\n              7,\n              12\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.58288770053475936,\n              \"x1\" : 0.49686520376175547,\n              \"x2\" : 0.5650470219435737,\n              \"y2\" : 0.56149732620320858\n            },\n            \"text\" : \"BLACK\"\n          },\n          {\n            \"text\" : \"damage\",\n            \"offset\" : [\n              13,\n              19\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.63087774294670851,\n              \"x1\" : 0.56739811912225702,\n              \"y1\" : 0.58288770053475936,\n              \"y2\" : 0.56149732620320858\n            }\n          },\n          {\n            \"offset\" : [\n              20,\n              26\n            ],\n            \"text\" : \"threat\",\n            \"bounds\" : {\n              \"y2\" : 0.56149732620320858,\n              \"x1\" : 0.63322884012539182,\n              \"y1\" : 0.58288770053475936,\n              \"x2\" : 0.68260188087774298\n            }\n          },\n          {\n            \"text\" : \"of\",\n            \"bounds\" : {\n              \"x2\" : 0.70376175548589337,\n              \"y2\" : 0.56149732620320858,\n              \"x1\" : 0.6849529780564263,\n              \"y1\" : 0.58288770053475936\n            },\n            \"offset\" : [\n              27,\n              29\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"x1\" : 0.7061128526645768,\n              \"y1\" : 0.58288770053475936,\n              \"x2\" : 0.77272727272727271,\n              \"y2\" : 0.56149732638146166\n            },\n            \"offset\" : [\n              30,\n              35\n            ],\n            \"text\" : \"FOWL.\"\n          }\n        ],\n        \"bounds\" : {\n          \"y1\" : 0.5828877005602241,\n          \"x2\" : 0.77272727520152262,\n          \"x1\" : 0.42633229087550378,\n          \"y2\" : 0.56149732622867332\n        },\n        \"text\" : \"BEARS, BLACK damage threat of FOWL.\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"CHICKENS (OTHER)\",\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"text\" : \"CHICKENS\",\n            \"offset\" : [\n              0,\n              8\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.5329153605015674,\n              \"x1\" : 0.42476489028213166,\n              \"y1\" : 0.55793226352941172,\n              \"y2\" : 0.53654188948306603\n            }\n          },\n          {\n            \"offset\" : [\n              9,\n              16\n            ],\n            \"text\" : \"(OTHER)\",\n            \"bounds\" : {\n              \"y1\" : 0.55793226381461669,\n              \"x1\" : 0.53526645768025083,\n              \"x2\" : 0.62068965125391862,\n              \"y2\" : 0.5365418896256684\n            }\n          }\n        ],\n        \"bounds\" : {\n          \"y1\" : 0.5579322637433155,\n          \"x2\" : 0.62068965321316616,\n          \"x1\" : 0.42476488832288406,\n          \"y2\" : 0.53654188941176473\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              4\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.49019607843137258,\n              \"y1\" : 0.51336898349376114,\n              \"x1\" : 0.42633228912225701,\n              \"x2\" : 0.46963165826558317\n            },\n            \"text\" : \"Cmp:\"\n          },\n          {\n            \"text\" : \"PHYSICAL\",\n            \"bounds\" : {\n              \"x1\" : 0.47217868002230845,\n              \"y2\" : 0.49019607843137258,\n              \"x2\" : 0.57151252853459322,\n              \"y1\" : 0.5133689839572193\n            },\n            \"offset\" : [\n              5,\n              13\n            ]\n          },\n          {\n            \"offset\" : [\n              14,\n              21\n            ],\n            \"text\" : \"ACTIONS\",\n            \"bounds\" : {\n              \"x1\" : 0.5740595502913185,\n              \"y2\" : 0.49019607843137258,\n              \"y1\" : 0.5133689839572193,\n              \"x2\" : 0.660658290019977\n            }\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.5133689839572193,\n              \"y2\" : 0.49019607843137258,\n              \"x2\" : 0.80094043210031363,\n              \"x1\" : 0.66320531177670228\n            },\n            \"offset\" : [\n              22,\n              34\n            ],\n            \"text\" : \"(HAND\\/VOICE)\"\n          }\n        ],\n        \"bounds\" : {\n          \"y1\" : 0.51336898372549022,\n          \"x2\" : 0.80094043584639507,\n          \"x1\" : 0.42633228537617562,\n          \"y2\" : 0.4901960781996435\n        },\n        \"text\" : \"Cmp: PHYSICAL ACTIONS (HAND\\/VOICE)\",\n        \"confidence\" : 1\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"APPLIED\\/USED 1 IN\",\n        \"bounds\" : {\n          \"x1\" : 0.4263322896663681,\n          \"x2\" : 0.60344827712718319,\n          \"y1\" : 0.48663101614463966,\n          \"y2\" : 0.46880570420168066\n        },\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              12\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.42633229093148228,\n              \"x2\" : 0.56543887147335425,\n              \"y1\" : 0.48663101601731606,\n              \"y2\" : 0.4688057040998217\n            },\n            \"text\" : \"APPLIED\\/USED\"\n          },\n          {\n            \"text\" : \"1\",\n            \"offset\" : [\n              13,\n              14\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.56739811912225702,\n              \"y1\" : 0.4866310160427807,\n              \"x2\" : 0.57915360501567403,\n              \"y2\" : 0.4688057040998217\n            }\n          },\n          {\n            \"bounds\" : {\n              \"x2\" : 0.60344827586206895,\n              \"y2\" : 0.46880570432900426,\n              \"x1\" : 0.5811128526645768,\n              \"y1\" : 0.4866310160427807\n            },\n            \"text\" : \"IN\",\n            \"offset\" : [\n              15,\n              17\n            ]\n          }\n        ],\n        \"confidence\" : 1\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"x2\" : 0.85109717625391856,\n          \"x1\" : 0.48432601637931039,\n          \"y2\" : 0.44028520481283429,\n          \"y1\" : 0.46167557914438506\n        },\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"y1\" : 0.46167557896613198,\n              \"x1\" : 0.48432601943573667,\n              \"x2\" : 0.52429467084639503,\n              \"y2\" : 0.44028520499108736\n            },\n            \"offset\" : [\n              0,\n              3\n            ],\n            \"text\" : \"Cmp\"\n          },\n          {\n            \"bounds\" : {\n              \"x2\" : 0.57366771159874608,\n              \"x1\" : 0.52664576802507834,\n              \"y2\" : 0.44028520499108736,\n              \"y1\" : 0.46167557932263814\n            },\n            \"text\" : \"Take:\",\n            \"offset\" : [\n              4,\n              9\n            ]\n          },\n          {\n            \"text\" : \"1\",\n            \"bounds\" : {\n              \"x1\" : 0.5760188087774295,\n              \"y2\" : 0.44028520499108736,\n              \"y1\" : 0.46167557932263814,\n              \"x2\" : 0.58777429467084641\n            },\n            \"offset\" : [\n              10,\n              11\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"y2\" : 0.44028520499108736,\n              \"y1\" : 0.46167557932263814,\n              \"x1\" : 0.59012539184952983,\n              \"x2\" : 0.61833855799373039\n            },\n            \"offset\" : [\n              12,\n              14\n            ],\n            \"text\" : \"EA\"\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.46167557932263814,\n              \"x1\" : 0.62068965517241381,\n              \"y2\" : 0.44028520499108736,\n              \"x2\" : 0.74294670846394983\n            },\n            \"text\" : \"BEARBLACK\",\n            \"offset\" : [\n              15,\n              24\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"x1\" : 0.74529780564263326,\n              \"y1\" : 0.46167557932263814,\n              \"x2\" : 0.85109717319749223,\n              \"y2\" : 0.44028520499108736\n            },\n            \"text\" : \"DISPERSED\",\n            \"offset\" : [\n              25,\n              34\n            ]\n          }\n        ],\n        \"text\" : \"Cmp Take: 1 EA BEARBLACK DISPERSED\",\n        \"confidence\" : 0.5\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 0.30000001192092896,\n        \"bounds\" : {\n          \"y1\" : 0.43493761126814368,\n          \"x2\" : 0.54702194394312587,\n          \"y2\" : 0.41532976813088873,\n          \"x1\" : 0.48275862105911332\n        },\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"y1\" : 0.4349376111280876,\n              \"x2\" : 0.50862068965517238,\n              \"y2\" : 0.4153297682709447,\n              \"x1\" : 0.48275862151813703\n            },\n            \"text\" : \"Int\",\n            \"offset\" : [\n              0,\n              3\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.43493761140819964,\n              \"y2\" : 0.4153297682709447,\n              \"x2\" : 0.54702194348410205,\n              \"x1\" : 0.51077586206896552\n            },\n            \"text\" : \"Trgt\",\n            \"offset\" : [\n              4,\n              8\n            ]\n          }\n        ],\n        \"text\" : \"Int Trgt\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"x1\" : 0.42476488655172423,\n          \"y2\" : 0.38859180024955453,\n          \"x2\" : 0.79780563890282141,\n          \"y1\" : 0.41354723696969709\n        },\n        \"confidence\" : 1,\n        \"text\" : \"Cmp: BARRIERS, FENCING (PERMANENT\",\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"x2\" : 0.46865203462798022,\n              \"x1\" : 0.42476489028213166,\n              \"y1\" : 0.4135472367201426,\n              \"y2\" : 0.3885918003565062\n            },\n            \"offset\" : [\n              0,\n              4\n            ],\n            \"text\" : \"Cmp:\"\n          },\n          {\n            \"text\" : \"BARRIERS,\",\n            \"offset\" : [\n              5,\n              14\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.47139498114959572,\n              \"y2\" : 0.3885918003565062,\n              \"y1\" : 0.41354723707664887,\n              \"x2\" : 0.57562694897098599\n            }\n          },\n          {\n            \"text\" : \"FENCING\",\n            \"offset\" : [\n              15,\n              22\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.3885918003565062,\n              \"x2\" : 0.66614418418429866,\n              \"y1\" : 0.41354723707664887,\n              \"x1\" : 0.57836989549260154\n            }\n          },\n          {\n            \"offset\" : [\n              23,\n              33\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.3885918004991088,\n              \"x1\" : 0.6688871307059141,\n              \"y1\" : 0.41354723707664887,\n              \"x2\" : 0.79780563517241398\n            },\n            \"text\" : \"(PERMANENT\"\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"x1\" : 0.4247648866300941,\n          \"x2\" : 0.78996864838558001,\n          \"y2\" : 0.36541889474153288,\n          \"y1\" : 0.38502673787878783\n        },\n        \"confidence\" : 1,\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              10\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.42476489028213166,\n              \"y2\" : 0.36541889483065959,\n              \"y1\" : 0.38502673768270945,\n              \"x2\" : 0.5540752351097179\n            },\n            \"text\" : \"ELECTRICAL\"\n          },\n          {\n            \"offset\" : [\n              11,\n              23\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.55623040752351094,\n              \"x2\" : 0.69631661442006265,\n              \"y1\" : 0.38502673796791442,\n              \"y2\" : 0.36541889483065959\n            },\n            \"text\" : \"APPLIED\\/USED\"\n          },\n          {\n            \"text\" : \"12\",\n            \"offset\" : [\n              24,\n              26\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.36541889483065959,\n              \"x1\" : 0.6984717868338558,\n              \"y1\" : 0.38502673796791442,\n              \"x2\" : 0.72002351097178685\n            }\n          },\n          {\n            \"offset\" : [\n              27,\n              30\n            ],\n            \"text\" : \"LIN\",\n            \"bounds\" : {\n              \"x1\" : 0.72217868338557989,\n              \"y1\" : 0.38502673796791442,\n              \"x2\" : 0.75881661442006265,\n              \"y2\" : 0.36541889483065959\n            }\n          },\n          {\n            \"offset\" : [\n              31,\n              33\n            ],\n            \"text\" : \"YD\",\n            \"bounds\" : {\n              \"x1\" : 0.7609717868338558,\n              \"y1\" : 0.38502673796791442,\n              \"x2\" : 0.78996864473354245,\n              \"y2\" : 0.36541889493761137\n            }\n          }\n        ],\n        \"text\" : \"ELECTRICAL APPLIED\\/USED 12 LIN YD\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"text\" : \"YOUNG\",\n            \"offset\" : [\n              0,\n              5\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.49745297805642635,\n              \"y1\" : 0.34046345809071099,\n              \"x1\" : 0.42633229292929287,\n              \"y2\" : 0.32085561497326198\n            }\n          },\n          {\n            \"offset\" : [\n              6,\n              11\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.49960815047021945,\n              \"y1\" : 0.34046345811051693,\n              \"y2\" : 0.32085561497326198,\n              \"x2\" : 0.56857366771159878\n            },\n            \"text\" : \"BLACK\"\n          },\n          {\n            \"text\" : \"BEAR\",\n            \"offset\" : [\n              12,\n              16\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.62676332288401249,\n              \"y1\" : 0.34046345811051693,\n              \"y2\" : 0.32085561497326198,\n              \"x1\" : 0.57072884012539182\n            }\n          },\n          {\n            \"offset\" : [\n              17,\n              21\n            ],\n            \"text\" : \"CAME\",\n            \"bounds\" : {\n              \"x1\" : 0.62891849529780564,\n              \"y1\" : 0.34046345811051693,\n              \"y2\" : 0.32085561497326198,\n              \"x2\" : 0.68926332288401249\n            }\n          },\n          {\n            \"text\" : \"INTO\",\n            \"offset\" : [\n              22,\n              26\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.34046345811051693,\n              \"x2\" : 0.74098746081504707,\n              \"y2\" : 0.32085561497326198,\n              \"x1\" : 0.69141849529780564\n            }\n          },\n          {\n            \"bounds\" : {\n              \"x1\" : 0.74314263322884011,\n              \"y1\" : 0.34046345811051693,\n              \"y2\" : 0.32085561497326198,\n              \"x2\" : 0.76469435736677116\n            },\n            \"text\" : \"50\",\n            \"offset\" : [\n              27,\n              29\n            ]\n          },\n          {\n            \"offset\" : [\n              30,\n              35\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.34046345811051693,\n              \"x2\" : 0.83385579937304077,\n              \"x1\" : 0.76684952978056431,\n              \"y2\" : 0.320855615171321\n            },\n            \"text\" : \"YARDS\"\n          }\n        ],\n        \"text\" : \"YOUNG BLACK BEAR CAME INTO 50 YARDS\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x1\" : 0.42633229066527339,\n          \"y1\" : 0.34046345819964341,\n          \"y2\" : 0.32085561506238858,\n          \"x2\" : 0.8338558016370603\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"bounds\" : {\n              \"x1\" : 0.42633229851097154,\n              \"y1\" : 0.31372548966131908,\n              \"x2\" : 0.4898119122257053,\n              \"y2\" : 0.29233511586452765\n            },\n            \"text\" : \"WHILE\",\n            \"offset\" : [\n              0,\n              5\n            ]\n          },\n          {\n            \"text\" : \"I\",\n            \"bounds\" : {\n              \"y1\" : 0.31372549019607843,\n              \"y2\" : 0.29233511586452765,\n              \"x1\" : 0.49216300940438873,\n              \"x2\" : 0.50156739811912221\n            },\n            \"offset\" : [\n              6,\n              7\n            ]\n          },\n          {\n            \"offset\" : [\n              8,\n              11\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.50391849529780564,\n              \"y2\" : 0.29233511586452765,\n              \"y1\" : 0.31372549019607843,\n              \"x2\" : 0.55094043887147337\n            },\n            \"text\" : \"WAS\"\n          },\n          {\n            \"text\" : \"WORKING.\",\n            \"bounds\" : {\n              \"x1\" : 0.55329153605015668,\n              \"y1\" : 0.31372549019607843,\n              \"y2\" : 0.29233511586452765,\n              \"x2\" : 0.65438871473354232\n            },\n            \"offset\" : [\n              12,\n              20\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"x1\" : 0.65673981191222575,\n              \"y1\" : 0.31372549019607843,\n              \"x2\" : 0.66614420062695923,\n              \"y2\" : 0.29233511586452765\n            },\n            \"offset\" : [\n              21,\n              22\n            ],\n            \"text\" : \"I\"\n          },\n          {\n            \"offset\" : [\n              23,\n              28\n            ],\n            \"text\" : \"HAZED\",\n            \"bounds\" : {\n              \"x1\" : 0.66849529780564265,\n              \"y1\" : 0.31372549019607843,\n              \"x2\" : 0.73667711598746077,\n              \"y2\" : 0.29233511586452765\n            }\n          },\n          {\n            \"bounds\" : {\n              \"y2\" : 0.29233511586452765,\n              \"x2\" : 0.7601880877742947,\n              \"x1\" : 0.7390282131661442,\n              \"y1\" : 0.31372549019607843\n            },\n            \"text\" : \"IT\",\n            \"offset\" : [\n              29,\n              31\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"y2\" : 0.29233511586452765,\n              \"y1\" : 0.31372549019607843,\n              \"x2\" : 0.83228840121473358,\n              \"x1\" : 0.76253918495297801\n            },\n            \"text\" : \"AWAY.\",\n            \"offset\" : [\n              32,\n              37\n            ]\n          }\n        ],\n        \"text\" : \"WHILE I WAS WORKING. I HAZED IT AWAY.\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y2\" : 0.29233511559714798,\n          \"x1\" : 0.42633229343652024,\n          \"x2\" : 0.83228840628918477,\n          \"y1\" : 0.31372548992869875\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"OPERATIONAL NONLETHAL PREDATION\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x2\" : 0.80721002818704279,\n          \"y1\" : 0.26737967905525839,\n          \"x1\" : 0.42789968335945666,\n          \"y2\" : 0.24598930472370761\n        },\n        \"subBounds\" : [\n          {\n            \"text\" : \"OPERATIONAL\",\n            \"bounds\" : {\n              \"x1\" : 0.42789968652037619,\n              \"x2\" : 0.56661442006269591,\n              \"y2\" : 0.24598930481283421,\n              \"y1\" : 0.26737967887700531\n            },\n            \"offset\" : [\n              0,\n              11\n            ]\n          },\n          {\n            \"text\" : \"NONLETHAL\",\n            \"offset\" : [\n              12,\n              21\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.69122257053291536,\n              \"y1\" : 0.26737967914438499,\n              \"y2\" : 0.24598930481283421,\n              \"x1\" : 0.56896551724137934\n            }\n          },\n          {\n            \"text\" : \"PREDATION\",\n            \"offset\" : [\n              22,\n              31\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.69357366771159878,\n              \"x2\" : 0.80721002502612338,\n              \"y2\" : 0.2459893049019608,\n              \"y1\" : 0.26737967914438499\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"text\" : \"DAMAGE MANAGEMENT\",\n        \"subBounds\" : [\n          {\n            \"text\" : \"DAMAGE\",\n            \"offset\" : [\n              0,\n              6\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.22103386809269165,\n              \"y1\" : 0.23885917998472117,\n              \"x2\" : 0.51763322884012541,\n              \"x1\" : 0.42946708802060007\n            }\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.23885918003565065,\n              \"y2\" : 0.22103386829640947,\n              \"x2\" : 0.66614420062695923,\n              \"x1\" : 0.51959247648902818\n            },\n            \"offset\" : [\n              7,\n              17\n            ],\n            \"text\" : \"MANAGEMENT\"\n          }\n        ],\n        \"bounds\" : {\n          \"y1\" : 0.23885918011204488,\n          \"x2\" : 0.66614420231751004,\n          \"y2\" : 0.22103386816908588,\n          \"x1\" : 0.42946708633004926\n        }\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x2\" : 0.38871473156739816,\n          \"y2\" : 0.13190730832442066,\n          \"x1\" : 0.19122256855799377,\n          \"y1\" : 0.15508021385026738\n        },\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              8\n            ],\n            \"text\" : \"FlaggedX\",\n            \"bounds\" : {\n              \"x1\" : 0.19122257053291536,\n              \"y1\" : 0.1550802136185383,\n              \"x2\" : 0.27272726674812342,\n              \"y2\" : 0.13190730837789666\n            }\n          },\n          {\n            \"text\" : \"by:\",\n            \"bounds\" : {\n              \"y1\" : 0.15508021390374327,\n              \"x1\" : 0.27527428850484864,\n              \"x2\" : 0.3032915278288264,\n              \"y2\" : 0.13190730837789666\n            },\n            \"offset\" : [\n              9,\n              12\n            ]\n          },\n          {\n            \"offset\" : [\n              13,\n              22\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.38871472959247655,\n              \"x1\" : 0.30583854958555168,\n              \"y1\" : 0.15508021390374327,\n              \"y2\" : 0.13190730855614974\n            },\n            \"text\" : \"Alexandra\"\n          }\n        ],\n        \"text\" : \"FlaggedX by: Alexandra\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"Few on 07\\/18\\/19\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x2\" : 0.32445141098484848,\n          \"y2\" : 0.11051693420974451,\n          \"y1\" : 0.13012477734699945,\n          \"x1\" : 0.19122257085945663\n        },\n        \"subBounds\" : [\n          {\n            \"text\" : \"Few\",\n            \"bounds\" : {\n              \"x2\" : 0.22786050156739812,\n              \"y1\" : 0.13012477718360071,\n              \"y2\" : 0.11051693404634577,\n              \"x1\" : 0.19122257196969694\n            },\n            \"offset\" : [\n              0,\n              3\n            ]\n          },\n          {\n            \"text\" : \"on\",\n            \"offset\" : [\n              4,\n              6\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.11051693404634577,\n              \"y1\" : 0.13012477718360071,\n              \"x1\" : 0.23001567398119122,\n              \"x2\" : 0.25156739811912227\n            }\n          },\n          {\n            \"text\" : \"07\\/18\\/19\",\n            \"bounds\" : {\n              \"y1\" : 0.13012477718360071,\n              \"x1\" : 0.25372257053291536,\n              \"y2\" : 0.11051693437314314,\n              \"x2\" : 0.32445140987460813\n            },\n            \"offset\" : [\n              7,\n              15\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              9\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.074866310160427774,\n              \"y1\" : 0.10516934046345816,\n              \"x2\" : 0.26782915360501569,\n              \"x1\" : 0.19122257339341686\n            },\n            \"text\" : \"Corrected\"\n          },\n          {\n            \"text\" : \"by\",\n            \"offset\" : [\n              10,\n              12\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.10516934046345816,\n              \"x2\" : 0.28781347962382448,\n              \"y2\" : 0.074866310160427774,\n              \"x1\" : 0.2711598746081505\n            }\n          },\n          {\n            \"text\" : \"°\",\n            \"offset\" : [\n              13,\n              14\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.3077978056426332,\n              \"y1\" : 0.10516934046345816,\n              \"x1\" : 0.29114420062695923,\n              \"y2\" : 0.074866310160427774\n            }\n          },\n          {\n            \"text\" : \"0\\/\",\n            \"offset\" : [\n              15,\n              17\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.10516934046345816,\n              \"x1\" : 0.31112852664576801,\n              \"x2\" : 0.33777429467084641,\n              \"y2\" : 0.074866310160427774\n            }\n          },\n          {\n            \"text\" : \"a\",\n            \"bounds\" : {\n              \"y2\" : 0.074866310160427774,\n              \"y1\" : 0.10516934046345816,\n              \"x2\" : 0.34443573667711597,\n              \"x1\" : 0.34110501567398122\n            },\n            \"offset\" : [\n              18,\n              19\n            ]\n          },\n          {\n            \"text\" : \"va\",\n            \"offset\" : [\n              20,\n              22\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.074866310918003443,\n              \"x1\" : 0.34776645768025077,\n              \"y1\" : 0.10516934046345816,\n              \"x2\" : 0.3526645756269593\n            }\n          }\n        ],\n        \"bounds\" : {\n          \"x2\" : 0.35266457764498432,\n          \"y2\" : 0.074866310539215775,\n          \"y1\" : 0.10516934084224594,\n          \"x1\" : 0.19122257137539184\n        },\n        \"confidence\" : 0.30000001192092896,\n        \"text\" : \"Corrected by ° 0\\/ a va\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y1\" : 0.074769308303107351,\n          \"x2\" : 0.33387747120885075,\n          \"y2\" : 0.053572937846799396,\n          \"x1\" : 0.26016641675669006\n        },\n        \"text\" : \"07\\/19\\/19\",\n        \"subBounds\" : [\n          {\n            \"text\" : \"07\\/19\\/19\",\n            \"offset\" : [\n              0,\n              8\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.074769308397636292,\n              \"y2\" : 0.053572937752270122,\n              \"x2\" : 0.33387747148615882,\n              \"x1\" : 0.26016641647938205\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"text\" : \"Please add the component hazing to this work task and\",\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"x1\" : 0.41692789699255495,\n          \"y1\" : 0.15508021384803916,\n          \"x2\" : 0.84796237975117572,\n          \"y2\" : 0.13190730832219244\n        },\n        \"subBounds\" : [\n          {\n            \"text\" : \"Please\",\n            \"offset\" : [\n              0,\n              6\n            ],\n            \"bounds\" : {\n              \"x2\" : 0.46786833482102541,\n              \"x1\" : 0.41692789968652039,\n              \"y1\" : 0.15508021370320857,\n              \"y2\" : 0.13190730837789666\n            }\n          },\n          {\n            \"offset\" : [\n              7,\n              10\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.13190730837789666,\n              \"x1\" : 0.47041535657775063,\n              \"y1\" : 0.15508021390374327,\n              \"x2\" : 0.50097961765845367\n            },\n            \"text\" : \"add\"\n          },\n          {\n            \"offset\" : [\n              11,\n              14\n            ],\n            \"text\" : \"the\",\n            \"bounds\" : {\n              \"y1\" : 0.15508021390374327,\n              \"y2\" : 0.13190730837789666,\n              \"x1\" : 0.50352663941517894,\n              \"x2\" : 0.52899685698243148\n            }\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.15508021390374327,\n              \"x2\" : 0.6232366619812657,\n              \"x1\" : 0.53154387873915665,\n              \"y2\" : 0.13190730837789666\n            },\n            \"offset\" : [\n              15,\n              24\n            ],\n            \"text\" : \"component\"\n          },\n          {\n            \"bounds\" : {\n              \"y2\" : 0.13190730837789666,\n              \"x2\" : 0.67927114062922123,\n              \"y1\" : 0.15508021390374327,\n              \"x1\" : 0.62578368373799098\n            },\n            \"offset\" : [\n              25,\n              31\n            ],\n            \"text\" : \"hazing\"\n          },\n          {\n            \"text\" : \"to\",\n            \"offset\" : [\n              32,\n              34\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.13190730837789666,\n              \"x1\" : 0.6818181623859465,\n              \"y1\" : 0.15508021390374327,\n              \"x2\" : 0.69964731468302332\n            }\n          },\n          {\n            \"offset\" : [\n              35,\n              39\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.70219433643974849,\n              \"y1\" : 0.15508021390374327,\n              \"x2\" : 0.73275859752045158,\n              \"y2\" : 0.13190730837789666\n            },\n            \"text\" : \"this\"\n          },\n          {\n            \"offset\" : [\n              40,\n              44\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.15508021390374327,\n              \"x2\" : 0.77860498914150611,\n              \"x1\" : 0.73530561927717686,\n              \"y2\" : 0.13190730837789666\n            },\n            \"text\" : \"work\"\n          },\n          {\n            \"offset\" : [\n              45,\n              49\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.13190730837789666,\n              \"x1\" : 0.78115201089823139,\n              \"x2\" : 0.81426329373565964,\n              \"y1\" : 0.15508021390374327\n            },\n            \"text\" : \"task\"\n          },\n          {\n            \"offset\" : [\n              50,\n              53\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.13190730846702314,\n              \"x2\" : 0.84796237705721011,\n              \"x1\" : 0.81681031549238481,\n              \"y1\" : 0.15508021390374327\n            },\n            \"text\" : \"and\"\n          }\n        ]\n      }\n    },\n    {\n      \"observation\" : {\n        \"subBounds\" : [\n          {\n            \"offset\" : [\n              0,\n              8\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.13198302617439839,\n              \"x1\" : 0.41537440775719958,\n              \"y2\" : 0.10641790388710937,\n              \"x2\" : 0.4782731557767686\n            },\n            \"text\" : \"indicate\"\n          },\n          {\n            \"bounds\" : {\n              \"x1\" : 0.48120274996350854,\n              \"x2\" : 0.51941586255074168,\n              \"y1\" : 0.13134588885603504,\n              \"y2\" : 0.10601969325351657\n            },\n            \"text\" : \"how\",\n            \"offset\" : [\n              9,\n              12\n            ]\n          },\n          {\n            \"offset\" : [\n              13,\n              16\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.10570112474664217,\n              \"x2\" : 0.55233002796992026,\n              \"x1\" : 0.52234545673748167,\n              \"y1\" : 0.13094767822244213\n            },\n            \"text\" : \"you\"\n          },\n          {\n            \"bounds\" : {\n              \"x2\" : 0.60170127609868795,\n              \"y2\" : 0.10522327198633075,\n              \"y1\" : 0.13062910971556796,\n              \"x1\" : 0.55525962215666014\n            },\n            \"text\" : \"hazed\",\n            \"offset\" : [\n              17,\n              22\n            ]\n          },\n          {\n            \"offset\" : [\n              23,\n              26\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.60463087028542783,\n              \"y2\" : 0.1049312508550293,\n              \"x2\" : 0.63187259439960153,\n              \"y1\" : 0.13015125695525642\n            },\n            \"text\" : \"the\"\n          },\n          {\n            \"text\" : \"bear\",\n            \"offset\" : [\n              27,\n              31\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.12985923582395498,\n              \"x2\" : 0.67027245405530977,\n              \"y2\" : 0.10455958759700934,\n              \"x1\" : 0.63480218858634141\n            }\n          },\n          {\n            \"bounds\" : {\n              \"y2\" : 0.10405518746112496,\n              \"x1\" : 0.67320204824204966,\n              \"x2\" : 0.72238654930234236,\n              \"y1\" : 0.12948757256593491\n            },\n            \"offset\" : [\n              32,\n              37\n            ],\n            \"text\" : \"away.\"\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.12898317243005064,\n              \"x1\" : 0.72531614348908224,\n              \"x2\" : 0.79148995940014022,\n              \"y2\" : 0.1025756349286161\n            },\n            \"offset\" : [\n              38,\n              45\n            ],\n            \"text\" : \"Thanks!\"\n          }\n        ],\n        \"confidence\" : 1,\n        \"bounds\" : {\n          \"y1\" : 0.13198302676332385,\n          \"x2\" : 0.79148995450205717,\n          \"y2\" : 0.10257563433969075,\n          \"x1\" : 0.41537441265528263\n        },\n        \"text\" : \"indicate how you hazed the bear away. Thanks!\"\n      }\n    },\n    {\n      \"observation\" : {\n        \"bounds\" : {\n          \"x1\" : 0.41692790228578885,\n          \"y1\" : 0.085561497400475361,\n          \"x2\" : 0.72884012799111808,\n          \"y2\" : 0.062388591874628641\n        },\n        \"confidence\" : 1,\n        \"text\" : \"Added Components and hazing activity\",\n        \"subBounds\" : [\n          {\n            \"text\" : \"Added\",\n            \"offset\" : [\n              0,\n              5\n            ],\n            \"bounds\" : {\n              \"y2\" : 0.062388591800356497,\n              \"x2\" : 0.47041535657775063,\n              \"x1\" : 0.41692790488505738,\n              \"y1\" : 0.085561497207367831\n            }\n          },\n          {\n            \"text\" : \"Components\",\n            \"offset\" : [\n              6,\n              16\n            ],\n            \"bounds\" : {\n              \"x1\" : 0.47296237833447591,\n              \"y1\" : 0.085561497326203217,\n              \"x2\" : 0.57484324860348601,\n              \"y2\" : 0.062388591800356497\n            }\n          },\n          {\n            \"text\" : \"and\",\n            \"offset\" : [\n              17,\n              20\n            ],\n            \"bounds\" : {\n              \"y1\" : 0.085561497326203217,\n              \"x2\" : 0.60795453144091427,\n              \"y2\" : 0.062388591800356497,\n              \"x1\" : 0.57739027036021118\n            }\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.085561497326203217,\n              \"y2\" : 0.062388591800356497,\n              \"x2\" : 0.66653603184559507,\n              \"x1\" : 0.61050155319763943\n            },\n            \"text\" : \"hazing\",\n            \"offset\" : [\n              21,\n              27\n            ]\n          },\n          {\n            \"bounds\" : {\n              \"y1\" : 0.085561497326203217,\n              \"x2\" : 0.7288401253918495,\n              \"y2\" : 0.06238859206773617,\n              \"x1\" : 0.66908305360232023\n            },\n            \"offset\" : [\n              28,\n              36\n            ],\n            \"text\" : \"activity\"\n          }\n        ]\n      }\n    }\n  ]\n}",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#do-the-math-to-assign-text-to-cell",
    "href": "presentation/index.html#do-the-math-to-assign-text-to-cell",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "Do the math to assign text to cell",
    "text": "Do the math to assign text to cell\n\n\nimport cv2\nimport json\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nimport glob\nfrom tqdm import tqdm\nimport os\nimport re\n\nfrom table_parsing.utils import stringify_keys\n\n# Handle input data\n# ==================================================\ndef process_image(image_path):\n\n    # Load the image\n    image = cv2.imread(image_path)\n\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    # Apply adaptive threshold to get binary image\n    binary = cv2.adaptiveThreshold(~gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, -10)\n    return image, binary\n\ndef load_data(json_path):\n    with open(json_path, 'r') as file:\n        data = json.load(file)\n\n    return(data)\n\n# Handle bounding boxes\n# ============================================\ndef draw_bounding_box(image, bounds, color=(0, 255, 0), thickness=2):\n    height, width, _ = image.shape\n    top_left = (int(bounds['x1'] * width), int((1 - bounds['y2']) * height))\n    bottom_right = (int(bounds['x2'] * width), int((1 - bounds['y1']) * height))\n    cv2.rectangle(image, top_left, bottom_right, color, thickness)\n\ndef convert_bounding_box(image, bounds):\n    height, width, _ = image.shape\n    # top left\n    x1 = int(bounds['x1'] * width)\n    y2 = int((1 - bounds['y2']) * height)\n    # Bottom right\n    x2 = int(bounds['x2'] * width)\n    y1 = int((1 - bounds['y1']) * height)\n\n    # Calculations\n    # width = x2 - x1\n    # height = y2 - y1\n\n    return x1, y1, x2, y2\n\ndef draw_all_word_bounding_boxes(image,data):\n    # Loop through the observations and subBounds to draw bounding boxes\n    for item in data['observations']:\n        # Draw bounding box for the entire phrase\n        draw_bounding_box(image, item[\"observation\"]['bounds'], color=(255, 0, 0))\n        # print(item[\"observation\"]['text'])\n    return(image)\n\n        # Draw bounding boxes for each word\n        # for subBound in item[\"observation\"]['subBounds']:\n        #     draw_bounding_box(image, subBound['bounds'], color=(0, 255, 0))\n\n    # # Display the image\n    # cv2.imshow('Image with Bounding Boxes', image)\n    # cv2.waitKey(0)\n    # cv2.destroyAllWindows()\n\n# Cluster points\n# ======================================\n\ndef find_horizontal_vertical_lines(binary, horizontal_scale_factor, vertical_scale_factor):\n    # # Parameters to adjust\n    # horizontal_scale_factor = 15  # Increase for larger structuring elements\n    # vertical_scale_factor = 50    # Increase for larger structuring elements\n    # dilation_size = 6            # Increase for more forgiving touch point detection\n\n    # Detect horizontal lines\n    horizontal = binary.copy()\n    cols = horizontal.shape[1]\n    horizontal_size = cols // horizontal_scale_factor\n    horizontal_structure = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))\n    horizontal = cv2.erode(horizontal, horizontal_structure)\n    horizontal = cv2.dilate(horizontal, horizontal_structure)\n\n    # Detect vertical lines\n    vertical = binary.copy()\n    rows = vertical.shape[0]\n    vertical_size = rows // vertical_scale_factor\n    vertical_structure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vertical_size))\n    vertical = cv2.erode(vertical, vertical_structure)\n    vertical = cv2.dilate(vertical, vertical_structure)\n\n    # Combine horizontal and vertical lines\n    # grid = cv2.add(horizontal, vertical)\n\n    return(horizontal, vertical)\n\n\n\n\n\n# Find touches (where lines touch but don't intersect)\ndef find_touches(image, horizontal, vertical, dilation_size):\n\n    # get touches\n    # ==============\n    touch_points = []\n\n    # Dilate horizontal and vertical lines to ensure touching points are detected\n    horizontal_dilated = cv2.dilate(horizontal, np.ones((dilation_size, dilation_size), np.uint8))\n    vertical_dilated = cv2.dilate(vertical, np.ones((dilation_size, dilation_size), np.uint8))\n    \n    # Combine dilated lines to find touch points\n    touch_points_img = cv2.bitwise_and(horizontal_dilated, vertical_dilated)\n    \n    # Get touch points coordinates\n    touch_points_coords = np.argwhere(touch_points_img == 255)\n    for point in touch_points_coords:\n        touch_points.append((point[1], point[0]))\n\n    # cluster touch points\n    # ======================\n    dbscan = DBSCAN(eps=10, min_samples=1).fit(touch_points)\n    clustered_touch_points = []\n    for label in np.unique(dbscan.labels_):\n        cluster = np.array(touch_points)[dbscan.labels_ == label]\n        if cluster.size &gt; 0:\n            clustered_touch_points.append(np.mean(cluster, axis=0).astype(int))\n\n\n    \n    return(clustered_touch_points)\n\n\n\ndef draw_clustered_touch_points(image, clustered_touch_points):\n    # Draw clustered touch points for visualization\n    for point in clustered_touch_points:\n        cv2.circle(image, tuple(point), 5, (255, 0, 0), -1)\n\n\n    # Display the image with detected lines, cells, intersections, and touches\n    return(image)\n\n\n# Final grid lines\n# ========================================\n\n# Identify and draw the outermost vertical and horizontal lines\ndef make_final_lines(image, clustered_touch_points):\n    if not clustered_touch_points:\n        return image\n\n    # touch_points_np = np.array(touch_points)\n    clustered_touch_points_np = np.array(clustered_touch_points)\n\n    # Identify the outermost vertical lines\n    left_most = np.min(clustered_touch_points_np[:, 0])\n    right_most = np.max(clustered_touch_points_np[:, 0])\n    middle = np.median(clustered_touch_points_np[:, 0])\n\n    # Identify the outermost horizontal lines\n    top_most = np.min(clustered_touch_points_np[:, 1])\n    bottom_most = np.max(clustered_touch_points_np[:, 1])\n\n\n\n    # Draw vertical lines\n    cv2.line(image, (left_most, 0), (left_most, image.shape[0]), (0, 255, 0), 2)\n    cv2.line(image, (int(middle), 0), (int(middle), image.shape[0]), (0, 255, 0), 2)\n    cv2.line(image, (right_most, 0), (right_most, image.shape[0]), (0, 255, 0), 2)\n\n    # Draw horizontal lines\n    cv2.line(image, (0, top_most), (image.shape[1], top_most), (0, 255, 0), 2)\n    cv2.line(image, (0, bottom_most), (image.shape[1], bottom_most), (0, 255, 0), 2)\n\n   \n\n    # Remove touch points that are within 5 pixels of top_most and bottom_most y-coordinates\n    filtered_touch_points = [point for point in clustered_touch_points if not (top_most - 5 &lt;= point[1] &lt;= top_most + 5 or bottom_most - 5 &lt;= point[1] &lt;= bottom_most + 5)]\n    filtered_touch_points_np = np.array(filtered_touch_points)\n\n    # Identify unique y-coordinates of filtered touch points\n    unique_y_coords = np.array(sorted(set(filtered_touch_points_np[:, 1]))).reshape(-1, 1)\n\n    # Cluster the y-coordinates using DBSCAN\n    dbscan = DBSCAN(eps=10, min_samples=1).fit(unique_y_coords)\n    clustered_y_coords = []\n\n    for label in np.unique(dbscan.labels_):\n        cluster = unique_y_coords[dbscan.labels_ == label]\n        if cluster.size &gt; 0:\n            clustered_y_coords.append(np.mean(cluster))\n\n    clustered_y_coords = sorted(clustered_y_coords)\n\n\n    \n    for y in clustered_y_coords:\n        y = int(y)\n\n        cv2.line(image, (left_most, y), (right_most, y), (0, 255, 0), 2)\n\n\n\n    return image, [top_most] + clustered_y_coords + [bottom_most], [left_most, middle, right_most]\n\n\n# Assigning data \n# ============================================\n\n# Function to calculate the intersection area of two rectangles\ndef intersection_area(box1, box2):\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n    return intersection\n\n\ndef assign_data(image, data, horizontal_lines, vertical_lines):\n\n    row_result = {}\n\n    for item in data['observations']:\n        \n        x1, y1, x2, y2 = convert_bounding_box(image, item[\"observation\"]['bounds'] )\n        text = item[\"observation\"]['text']\n\n\n        bounding_box = (x1, y1, x2, y2)\n\n\n        # Determine the cell containing the majority of the bounding box\n        max_intersection = 0\n        cell_with_max_intersection = (0, 0)\n\n        for i in range(len(horizontal_lines) - 1):\n            for j in range(len(vertical_lines) - 1):\n                # Define the bounding box for the current cell\n                cell_box = (vertical_lines[j], horizontal_lines[i], vertical_lines[j + 1], horizontal_lines[i + 1])\n                # print(\"bounding_box: \" + str(bounding_box))\n                # print(\"cell box: \" + str(cell_box))\n\n                \n                # Calculate the intersection area between the bounding box and the cell\n                intersection = intersection_area(bounding_box, cell_box)\n                # print(\"intersection: \" + str(intersection))\n                # Update the cell with the maximum intersection area\n                if intersection &gt; max_intersection:\n                    max_intersection = intersection\n                    cell_with_max_intersection = (i, j)\n\n        cell_key = cell_with_max_intersection\n        if cell_key in row_result:\n            row_result[cell_key] += \" \" + text\n        else:\n            row_result[cell_key] = text\n\n\n    return(row_result)\n\ndef natural_sort_key(s):\n    return [int(text) if text.isdigit() else text for text in re.split(r'(\\d+)', s)]\n\ndef make_result(image_dir, json_dir, jsonl_output_path, annotated_output_path):\n\n    image_paths =  sorted(glob.glob(f\"{image_dir}/*jpg\") , key=natural_sort_key)\n\n    json_paths = sorted(glob.glob(f\"{json_dir}/*json\"), key=natural_sort_key)\n\n\n    with open(jsonl_output_path, 'w') as jsonl_file:\n        for i in range(len(image_paths)):\n            image_path = image_paths[i]\n            json_path = json_paths[i]\n\n            file_name = os.path.basename(image_path)\n\n            image, binary = process_image(image_path)\n            data = load_data(json_path)\n\n            # image = draw_all_word_bounding_boxes(image, data)\n\n            horizontal_lines, vertical_lines = find_horizontal_vertical_lines(binary, horizontal_scale_factor = 15, vertical_scale_factor = 30)\n\n            clustered_touch_points = find_touches(image, horizontal_lines, vertical_lines, dilation_size=6)\n\n            image = draw_clustered_touch_points(image, clustered_touch_points)\n\n            image_with_lines, horizontal_lines, vertical_lines = make_final_lines(image, clustered_touch_points)\n\n            result = assign_data(image, data, horizontal_lines, vertical_lines)\n\n        \n\n            # Save results\n            # ============\n            cv2.imwrite(os.path.join(annotated_output_path, file_name), image_with_lines)\n            # Write the result to the JSONL file\n            jsonl_file.write(json.dumps(stringify_keys(result)) + '\\n')",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "presentation/index.html#a-table",
    "href": "presentation/index.html#a-table",
    "title": "Parsing Thousands of Scanned Pages",
    "section": "A table",
    "text": "A table",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parsing Thousands of Scanned Pages"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Intro‘That’s a bloodbath’: How a federal program kills wildlife for private interests Code\n\n\n\n\n\n‘That’s a bloodbath’: How a federal program kills wildlife for private interests\nThis methodology document shows the work behind each data sentence in the Wildlife Services story that can be read and listened to here.\nThe document is organized in the following manner:\n\nSection: Data Analysis\n\nData Sentences: Shows the work behind each data sentence\n\nSection: ETL (Extract Transform and Load)\n\nProcess PDR Report G\nScrape PDR Report G\nParse Scanned FOIAS\nParsed FOIA –&gt; CSV",
    "crumbs": [
      "Intro",
      "'That’s a bloodbath': How a federal program kills wildlife for private interests"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html",
    "href": "analysis/data-sentences.html",
    "title": "",
    "section": "",
    "text": "Data AnalysisData Sentences Methodology CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence",
    "href": "analysis/data-sentences.html#sentence",
    "title": "",
    "section": "Sentence",
    "text": "Sentence\n\n+digital: But today, Wildlife Services employees still kill hundreds of thousands of noninvasive animals a year, data from the agency shows. Even species considered threatened under the Endangered Species Act, like grizzly bears, are not exempt. So long as livestock or human life are threatened, federal rules allow Wildlife Services to kill those animals, too.\n\n\ndigital: But today, Wildlife Services employees still kill hundreds of thousands of noninvasive animals a year\n\n\n\nCode\nnon_is_animals_killed_df = all_killed_euth_df  %&gt;% \n2  filter(number_of_animals_killed_euthanized &gt; 0)  %&gt;%\n3  filter(invasive_status == \"non-is\")\n\n\nnon_is_animals_killed_df %&gt;% \n4  summarize(total_number_non_is_killed = sum(number_of_animals_killed_euthanized ),.by = year) %&gt;%\n  cat_table2(\"Number of non invasive (native) animals killed in 2023\")\n\n\n\n2\n\nFilter for just records that show that animals were killed or euthanized\n\n3\n\nFilter for noninvasive animals that were killed\n\n4\n\nAdd the number of animals killed\n\n\n\n\n\n\n\n\n\ndigital: Even species considered threatened under the Endangered Species Act, like grizzly bears, are not exempt.\n\n\n\nCode\nnon_is_animals_killed_df %&gt;% \n  filter(year == 2023) %&gt;% \n1  filter(species == \"bears, grizzly\") %&gt;%\n  cat_table2(\"Grizzly bears killed in 2023\")\n\n\n\n1\n\nFilter species for grizzly bears",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-1",
    "href": "analysis/data-sentences.html#sentence-1",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: An NPR analysis of those reports shows that Wildlife Services still killed more than 370,000 noninvasive animals across the country in the 2023 fiscal year.\n\n\n\n\nCode\n\nData source: PDR G Report\n\ninvasive_vs_noninvasive_killed_2023_df = all_killed_euth_df  %&gt;% \n1    filter(number_of_animals_killed_euthanized &gt; 0) %&gt;%\n2    filter(year == 2023) %&gt;%\n3    summarize(total_animals_killed = sum(number_of_animals_killed_euthanized), .by = c(species, invasive_status)) %&gt;%\n    summarize(number_killed = sum(total_animals_killed),\n    top_5_animals = paste(\n      glue(\"{species[order(-total_animals_killed)][1:5]} ({total_animals_killed[order(-total_animals_killed)][1:5]})\"),\n      collapse = \"| \"\n4    ), .by = c(invasive_status)) %&gt;%\n    arrange(desc(number_killed)) \n\n\ncat_table2(invasive_vs_noninvasive_killed_2023_df,  \"Federal workers mostly kill nonnative animals (Data from 2023)\")\n\n\n\n\n1\n\nFilter for just animals that were killed\n\n2\n\nFilter for year 2023\n\n3\n\nGet animals killed totals by species and invasive status\n\n4\n\nCreate a column that lists the top 5 animals killed per invasive status",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-2",
    "href": "analysis/data-sentences.html#sentence-2",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: And over the past nine years, Wildlife Services killed 30 threatened grizzly bears and at least 1,500 gray wolves in states where they were otherwise supposed to receive protection under the Endangered Species Act, like in Minnesota and Wisconsin.\n\n\n\n\n\n\n\n\n\nTimeline of Gray Wolf Protections\n\n\n\nWe arrived at it by looking at the state breakdown in the PDR G report and comparing it to this timeline.\n\n“March 9, 1978 – The Service issued a final rule reclassifying the gray wolf as endangered in Minnesota and threatened across the rest of the lower 48 states”.1\n“In 2011 Congress directed FWS to reinstate a rule delisting the gray wolf in the Northern Rocky Mountains other than Wyoming. FWS then delisted the gray wolf in Wyoming in 2012. After the U.S. District Court for the District of Columbia vacated the rule delisting the gray wolf in Wyoming in 2014, the U.S. Court of Appeals for the District of Columbia (D.C. Circuit) reinstated the rule in 2017.”2\n“Beginning in 2017, therefore, the gray wolf in the lower 48 states was divided into four separate groups for ESA purposes: (1) the Northern Rocky Mountain DPS was not listed; (2) gray wolves in Minnesota were listed as threatened; (3) the Mexican gray wolf subspecies in Arizona and New Mexico was listed as endangered beginning in 2015; and (4) gray wolves in all other areas of the lower 48 states were listed as endangered”3\n“Effective 2021-01-04, Trump administration removed all gray wolves from the endangered list and stripped their legal protections (except for the Mexican Wolf in the Southwestern Population)”4\n“On February 10, 2022, a court order vacated the November 3, 2020, rule that removed ESA protections, reinstating ESA protections for gray wolves in the lower 48 states, excluding the Northern Rocky Mountains.”5\n\n\n“Northern Rocky Mountain distinct population segment (Idaho, Montana, Wyoming, eastern one-third of Washington and Oregon, and a small portion of north central Utah.)”6\n\n\n\n\n\n\n\n\n\n\n\nTimeline of Grizzly Bear Protections\n\n\n\nGrizzly bears have been protected in the lower 48 since 19757\n\n\n\n\n\n\n\nCode\n\nData source: PDR G Report\n\nstate_killed_euth_df %&gt;% \n1  filter(species ==\"wolves, gray/timber\") %&gt;%\n  mutate(protected_status = case_when(\n\n2    year %in% c(2015,2016) & !(state %in% c(\"id\", \"mt\", \"wa\", \"or\", \"ut\")) ~ \"protected\",\n\n3    year %in% c(2017:2020) & !(state %in% c(\"id\", \"mt\", \"wa\", \"or\", \"wy\")) ~ \"protected\",\n\n    year %in% c(2021) ~ \"not_protected\", # &lt;4&gt;,\n\n5    year %in% c(2022) ~ \"not_protected\",\n\n    year %in% c(2023) & !(state %in% c(\"id\", \"mt\", \"wa\", \"or\", \"wy\")) ~ \"protected\",\n\n    TRUE ~ \"not_protected\")) %&gt;% \n  summarize(n = sum(n_killed_euth), .by = protected_status) %&gt;% \n  cat_table2(\"Wolves that were killed while under protection from 2015-2023\")\n\n\n\n\n1\n\nFilter just for Gray Wolves\n\n2\n\nFrom 2015 to the end of 2016: wolves were not protected in Idaho, Montana, Washington, Oregon or Utah (Northern Rocky population but not including Wyoming) and protected everywhere else because of the 2011 ruling.\n\n3\n\nFrom 2017 to 2021-01-04 : the entire Northern Rocky Population was not protected (since wolves delisted in Wyoming). In Minnesota the wolves were considered threatened and in the rest of the lower contiguous 48 they were endangered.\n\n5\n\nAll gray wolves regained protection except for the Northern Rocky Population. However the date they regained protection was February 10, 2022 and we cannot break down the PDR reports. Therefore while 139 were killed we cannot with 100% confidence say they were not killed before the 2022-02-10 date where they gained protection back.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nData source: PDR G Report\n\nstate_killed_euth_df %&gt;% \n1  filter(species ==\"bears, grizzly\") %&gt;%\n  mutate(protected_status = case_when(\n\n    !(state %in% c(\"ak\")) ~ \"protected\",\n    TRUE ~ \"not_protected\")) %&gt;% \n  summarize(n = sum(n_killed_euth), .by = protected_status) %&gt;% \n  cat_table2(\"Grizzly Bears that were killed while under protection from 2015-2023\")\n\n\n\n\n1\n\nFilter for grizzly bears",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-3",
    "href": "analysis/data-sentences.html#sentence-3",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: The documents reveal that during those three years, employees killed approximately 11,000 wild animals on Montana properties where no wildlife was recorded as responsible for killing or injuring any livestock.\n\n\n\n\nCode\n\nData source: FOIA EXCEL\n\nid_animal_conflict_df = conflict_df  %&gt;% \n  mutate(loss_qty = case_when(\n1    damage_threat == \"DISEASE THREAT\" ~ 1,\n    TRUE ~ loss_qty\n  ))  %&gt;% \n  summarize(number_livestock_loss = sum(loss_qty, na.rm = TRUE), n_times_recorded = n(), .by = c(id)) \n\nid_animal_take_df = take_df  %&gt;% \n  filter(fate == \"KILLED\" & uom == \"EACH\") %&gt;% \n  summarize(number_animals_killed = sum(qty), .by = c(id))\n\narea_action_summary_df = full_join(id_animal_conflict_df, id_animal_take_df, join_by(id)) %&gt;% \nmutate(across(where(is.numeric), ~replace_na(., 0))) %&gt;% \nmutate(category = case_when(\n  number_livestock_loss == 0 ~ \"no_livestock_loss_on_area_and_killed\",\n  TRUE ~ \"livestock_loss_and_killed\"\n))\n\nfinal_area_action_summary_df = area_action_summary_df  %&gt;% \n   summarize(total_animals_killed = sum(number_animals_killed), .by = c(category))  %&gt;% \n   pivot_wider(names_from = category, values_from = total_animals_killed, values_fill = 0 )  %&gt;%\n   mutate(total = no_livestock_loss_on_area_and_killed + livestock_loss_and_killed)  %&gt;% \n   mutate(pct_killed_no_livestock_loss_on_area = (no_livestock_loss_on_area_and_killed / total)*100) %&gt;% \n   arrange(desc(no_livestock_loss_on_area_and_killed))\n\nsum(final_area_action_summary_df$no_livestock_loss_on_area_and_killed)\n\n\n\n\n1\n\nTo be conservative, we removed cases where there was a disease threat logged for consideration\n\n\n\n\n[1] 10976",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-4",
    "href": "analysis/data-sentences.html#sentence-4",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: The agency frequently used helicopters and planes to shoot large numbers of wild animals at a time, the documents show, a method activists consider cruel and scientists say can lead to local eradications.\n\n\n\n\nCode\n\nData source: PDR G Report\n\ntake_df  %&gt;% \n  filter(fate == \"KILLED\") %&gt;% \n  filter(uom == \"EACH\") %&gt;% \n  summarize(number_animals_killed = sum(qty, na.rm = TRUE), .by = component) %&gt;% \n  arrange(desc(number_animals_killed)) %&gt;% \n  cat_table2(\"Number of Animals Killed By Method\")",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-5",
    "href": "analysis/data-sentences.html#sentence-5",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: By far, most of the thousands of animals Wildlife Services killed were coyotes, a species native to Montana.\n\n\n\n\nCode\n\nData source: FOIA Excel\n\nareas_no_livestock_loss_df = area_action_summary_df %&gt;% \n  filter(category == \"no_livestock_loss_on_area_and_killed\")\n\ntake_df  %&gt;% \n  filter(id %in% c(areas_no_livestock_loss_df$id)) %&gt;% \n  filter(fate == \"KILLED\") %&gt;% \n  summarize(animals_killed = sum(qty, na.rm = TRUE), .by = damage_agent) %&gt;% \n  arrange(desc(animals_killed)) %&gt;% \n  cat_table2(\"Livestock loss vs killed_analysis\")\n\n\n\n\n\n\n\n\nFilter for just animals killed at locations with no recorded livestock loss\nFilter for animals that were killed\nCalculate the number of animals for each species",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-6",
    "href": "analysis/data-sentences.html#sentence-6",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: At one location, Wildlife Services killed 318 coyotes — the most killed in any single area in Montana over those three years, according to the records. The documents did not contain any reports of coyotes killing livestock at that location over the same period of time.\n\n\ndigital: At one location, Wildlife Services killed 318 coyotes — the most killed in any single area in Montana over those three years, according to the records.\n\n\n\n\nCode\n\nData source: FOIA Excel\n\narea_no_livestock_loss_df = conflict_df  %&gt;% \n    mutate(loss_qty = case_when(\n1      damage_threat == \"DISEASE THREAT\" ~ 1,\n      TRUE ~ loss_qty\n    ))  %&gt;% \n    summarize(livestock_loss= sum(loss_qty, na.rm = TRUE), .by = id)  %&gt;% \n    filter(livestock_loss== 0)\n\ntop_coyote_area_no_livestock_loss_df = take_df  %&gt;% \n  filter(id %in% area_no_livestock_loss_df$id) %&gt;%\n2  filter(damage_agent == \"COYOTES\") %&gt;%\n3  filter(fate == \"KILLED\") %&gt;%\n4  summarize(animals_killed = sum(qty), .by = id) %&gt;%\n5  arrange(desc(animals_killed)) %&gt;%\n6  slice(1:5)\n\n\n\n\n1\n\nFilter for all areas where no livestock loss was recorded\n\n2\n\nFilter for only coyotes that are being killed\n\n3\n\nFilter for coyotes that were killed\n\n4\n\nTotal the number of coyotes killed by area\n\n5\n\nSort by descending order\n\n6\n\nTake the top 5 areas\n\n\n\n\n\n\nCode\ncat_table2(top_coyote_area_no_livestock_loss_df, \"Top 5 areas where coyotes were killed and no livestock loss was recorded\")\n\n\n\n\n\n\n\ndigital: The documents did not contain any reports of coyotes killing livestock at that location over the same period of time.\n\n\n\n\nCode\n\nData source: FOIA Excel\n\narea_high_coyote_df = conflict_df  %&gt;% \n1  filter(id == \"MT:404657\")\n\n\n\n\n1\n\nFilter conflict data for the area with the highest coyote killing\n\n2\n\nIf you go through all the pages of the table, there is no loss_qty due to coyotes\n\n\n\n\n\n\nCode\ncat_table2(area_high_coyote_df, \"area MT:404657 Conflict Loss\")",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-7",
    "href": "analysis/data-sentences.html#sentence-7",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: Montana is home to approximately 1,100 gray wolves. Over a span of three years, Wildlife Services killed 71 wolves at just five locations. During the same time, wolves were documented to have harmed 61 cattle and sheep in those areas. Since there were roughly 2.5 million cattle and sheep in Montana, this indicates that 6% of the state’s wolf population was killed for predation on just 0.002% of Montana’s livestock.\n\n\n\ndigital: Montana is home to approximately 1,100 gray wolves.\n\n\n\n\n\n\n\nNote\n\n\n\n2022 report: https://fwp.mt.gov/binaries/content/assets/fwp/conservation/wolf/draft-2022-wolf-report_final_6.21.23.pdf\n2021 report: https://fwp.mt.gov/binaries/content/assets/fwp/conservation/wolf/final-draft-2021-wolf-report-7.28.pdf\n2020 report: https://fwp.mt.gov/binaries/content/assets/fwp/conservation/wolf/annual-wolf-report-2020_.pdf\n2019 report: https://fwp.mt.gov/binaries/content/assets/fwp/conservation/wildlife-reports/wolf/2019-mt-wolf-annual-report-final-9.9.2020all.pdf\n\n\n\n\nCode\nmontana_wolf_population_df = tibble(\n  year = c(2022,2021,2020,2019),\n  population = c( 1087,1144, 1177, 833 ) \n) \n\nmontana_wolf_population_df  %&gt;% \n  summarize(average_population = mean(population))  %&gt;% \n  mutate(rounded_average_population = round(average_population, digits = -2)) %&gt;% \n  cat_table2(\"Average population of Gray Wolf in Montana from 2019-2022\")\n\n\n\n\n\n\n\ndigital: Over a span of three years, Wildlife Services killed 71 wolves at just five locations.\n\n\n\n\nCode\n\nData source: FOIA Excel\n\nwolves_killed_by_area_df = take_df  %&gt;% \n  filter(fate == \"KILLED\") %&gt;% \n  filter(damage_agent == \"WOLVES, GRAY/TIMBER\") %&gt;% \n  summarize(wolves_killed_on_area = sum(qty),.by = id) %&gt;% \n  mutate(pct = wolves_killed_on_area / sum(wolves_killed_on_area)*100) %&gt;% \n  arrange(desc( wolves_killed_on_area )) %&gt;% \n  mutate(cum_sum_wolves_killed = cumsum(wolves_killed_on_area),\n         cum_pct = cumsum(pct))\n\n\ncat_table2(wolves_killed_by_area_df, \"Total Wolves Killed From June 2019 - June 2022\")\n\n\n\n\n\n\n\n\ndigital: 6% of the state’s wolf population was killed\n\n\n\nCode\n# | output: asis\ntop_5_areas_number_wolves_killed = wolves_killed_by_area_df  %&gt;% \n  slice(1:5) %&gt;% \n  pull(wolves_killed_on_area) %&gt;% \n  sum()\n\nglue(\"The top 5 areas had {top_5_areas_number_wolves_killed} wolves killed by Wildlife Services from June 2019 to June 2022, **{round((top_5_areas_number_wolves_killed/1100)*100)}%** of their population\")\n\n\nThe top 5 areas had 71 wolves killed by Wildlife Services from June 2019 to June 2022, **6%** of their population\n\n\n\n\ndigital: During the same time, wolves were documented to have harmed 61 cattle and sheep in those areas. Since there were roughly 2.5 million cattle and sheep in Montana, this indicates that 6% of the state’s wolf population was killed for predation on just 0.002% of Montana’s livestock.\n\n\n\n\n\n\n\nStates Cattle and Sheep Population\n\n\n\n\nQuickstats report: https://quickstats.nass.usda.gov/results/2D2733DB-7F00-3144-84D4-94FF8C367655\n\nThe survey for each year is taken at January. That means that the year 2020 is a survey for 2019\n\n\nFrom USDA Acting Staff Director of Public Affairs Office via email:\n\n“The”CATTLE, INCL CALVES - INVENTORY” and “SHEEP, INCL LAMBS -INVENTORY” categories accurately represent the total number of cattle and sheep, respectively, for a given year.””\n\n\n\n\n\nCode\navg_sheep_cattle_population_df = sheep_cattle_populations_df  %&gt;% \n  summarize(cattle_sheep_population = sum(value), .by = year) %&gt;% \n  summarize(avg_cattle_sheep_population = mean(cattle_sheep_population)) %&gt;% \n  mutate(rounded_avg_cattle_sheep_population = round(avg_cattle_sheep_population, digits = -5))\n\ncat_table2(avg_sheep_cattle_population_df, \"Average Cattle and Sheep Population for 2019-2022\")\n\n\n\n\n\n\n\n\nCode\nlivestock_loss_by_wolves_on_top_5_area_df = conflict_df  %&gt;% \n  filter(id %in% wolves_killed_by_area_df[1:5, \"id\"][[1]]) %&gt;% \n  filter(damage_agent == \"WOLVES, GRAY/TIMBER\") %&gt;% \n  filter(loss_uom == \"EACH\") %&gt;% \n  summarize(livestock_loss_by_wolves = sum(loss_qty, na.rm = TRUE), .by = resource)\n\ncat_table2(livestock_loss_by_wolves_on_top_5_area_df, \"Livestock Killed By Wolves on Top 5 area\")\n\n\n\n\n\n\nCode\nsum(livestock_loss_by_wolves_on_top_5_area_df$livestock_loss_by_wolves)\n\n\n[1] 61\n\n\nCode\nlivestock_loss_by_wolves_on_top_5_area_df  %&gt;% \n  summarize(sheep_cattle_killed_by_wolves = sum(livestock_loss_by_wolves)) %&gt;% \n  mutate(avg_sheep_cattle_population = avg_sheep_cattle_population_df$rounded_avg_cattle_sheep_population[[1]]) %&gt;% \n  mutate(pct_sheep_cattle_killed =   (sheep_cattle_killed_by_wolves / avg_sheep_cattle_population)*100) %&gt;% \n  cat_table2(\"Pct of Cattle and Sheep Harmed By Wolves\")",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-8",
    "href": "analysis/data-sentences.html#sentence-8",
    "title": "",
    "section": "Sentence",
    "text": "Sentence\n\n+digital: But shooting from helicopters was the most common method in Montana, records show, and it was efficient. On average, every time Wildlife Services employees flew in a helicopter and killed coyotes, they shot six of the animals.\n\n\ndigital: But shooting from helicopters was Wildlife Services’ most commonly used method in Montana\n\n\n\nCode\ntake_df  %&gt;% \n  filter(fate == \"KILLED\") %&gt;% \n  filter(uom == \"EACH\") %&gt;% \n  summarize(number_animals_killed = sum(qty, na.rm = TRUE), .by = component) %&gt;% \n  arrange(desc(number_animals_killed)) %&gt;% \n  cat_table2(\"Number of Animals Killed By Method in Montana\")\n\n\n\n\n\n\n\ndigital: On average, every time Wildlife Services employees flew in a helicopter and killed coyotes, they shot six of the animals.\n\n\n\nCode\ntake_df  %&gt;% \n    filter(fate == \"KILLED\") %&gt;% \n    filter(uom == \"EACH\") %&gt;% \n    filter(component == \"HELICOPTER\")  %&gt;% \n    filter(damage_agent == \"COYOTES\")  %&gt;% \n    summarize(average_coyotoes_killed = mean(qty), median = median(qty), max_coyotes_killed = max(qty)) %&gt;% \n    cat_table2(\"Summary statisitics for coyotes killed with helicopter\")",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-9",
    "href": "analysis/data-sentences.html#sentence-9",
    "title": "",
    "section": "Sentence:",
    "text": "Sentence:\n\n+digital: At one location, federal employees shot and killed 61 coyotes in under four hours while flying in a helicopter, the documents reveal.\n\n\n\n\nCode\n\nData source: FOIA Excel\n\n# From excel sheets \ntake_df  %&gt;% \n  filter(id == \"MT:328609\") %&gt;% \n  filter(work_date == \"2020-04-20\") %&gt;% \n  cat_table2(\"61 Coyotes killed from Helicopter\")\n\n\n\n\n\n\n\nScreenshot of the scanned work task:\n\n\n\nScanned Work Task",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#sentence-10",
    "href": "analysis/data-sentences.html#sentence-10",
    "title": "",
    "section": "Sentence",
    "text": "Sentence\n\n+digital: But over the past five years, Congress allocated less than 2% of Wildlife Services’ wildlife management budget for its nonlethal livestock protection initiatives.\n\n\n\n\n\n\n\nNote\n\n\n\n\nClarification from WS via email that “agriculture funding” is where resources for lethal + nonlethal initiatives are pulled from: “The funding sources are on the left side of the graph at the bottom of the pie chart: Federal, Federal Cooperative, and Cooperative. Those four sources: Agriculture, Human Health and Safety, Property, and Natural Resources is what was protect. So for instance, the grand total for 2023 for agricultural protection was $79,190,404. This is for both lethal and nonlethal operations.”\nDenominator: Added up 2023, 2022, 2021, 2020, 2019 “Agriculture funding” totals, got $311,358,001\nNumerator: Emailed statement from WS to NPR: “While WS does not track expenditures by method or method type, WS has received $4.5M in Congressional directed funding for non-lethal livestock protection and beaver damage management over the last five years.”\n\n\n\n\n\nCode\nws_ag_funding_df = tibble(\n  year = c(2023, 2022, 2021, 2020,2019),\n  funding = c(79190404, 80597382, 53046582, 52066309,46457324 )\n)\n\n\nws_ag_funding_df  %&gt;% \n  summarize(total_5_years = sum(funding)) %&gt;% \n  mutate(non_lethal_livestock_protection = 4500000) %&gt;% \n  mutate(pct = (non_lethal_livestock_protection / total_5_years)*100) %&gt;% \n  cat_table2(\"Wildlife Services Budget Analysis\")",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "analysis/data-sentences.html#footnotes",
    "href": "analysis/data-sentences.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.biologicaldiversity.org/campaigns/gray_wolves/action_timeline.html#:~:text=January%204%2C%201974%20%E2%80%93%20The%20U.S.,of%20the%20lower%2048%20states↩︎\nhttps://crsreports.congress.gov/product/pdf/LSB/LSB10697↩︎\nhttps://crsreports.congress.gov/product/pdf/LSB/LSB10697↩︎\nhttps://crsreports.congress.gov/product/pdf/R/R46184, pg 29↩︎\nhttps://www.fws.gov/project/qas-western-gray-wolf-12-month-finding#:~:text=On%20February%2010%2C%202022%2C%20a%20court%20order%20vacated,lower%2048%20states%2C%20excluding%20the%20Northern%20Rocky%20Mountains.↩︎\nhttps://www.aphis.usda.gov/operational-wildlife-activities/livestock/gray-wolf-mountains#:~:text=In%202010%2C%20the%20minimum%20wolf,portion%20of%20north%20central%20Utah.↩︎\nhttps://www.nps.gov/noca/upload/NCE-Grizzly-Bear-EIS-FAQs-20221115.pdf#:~:text=%E2%80%A2%201975%20%E2%80%93%20Grizzly%20bear%20listed%20as%20threatened,%E2%80%A2%201983%20%E2%80%93%20Interagency%20Grizzly%20Bear%20Committee%20established.↩︎",
    "crumbs": [
      "Data Analysis",
      "Data Sentences Methodology"
    ]
  },
  {
    "objectID": "etl/1.process_g_table.html",
    "href": "etl/1.process_g_table.html",
    "title": "Process PDR G Report",
    "section": "",
    "text": "Code\nfrom bs4 import BeautifulSoup\nimport re\n\n\n\n\nCode\nwith open('../data/source/html/USDA-APHIS.html', 'r', encoding='utf-8') as file:\n    html_content = file.read()\n\nsoup = BeautifulSoup(html_content, 'html')\n\n\nUse webscraping to get the url codes for each state so we can use them to build all the urls\n\n\nCode\noptions = soup.find_all(\"option\")\n\n\n\nfor option in options:\n    input_string = option[\"value\"]\n    if \"STATE\" in input_string :\n        # State\n        # ==================\n        # Regex to find the state abbreviation following 'STATE:'\n        match = re.search(r\"STATE:(\\w\\w):\", input_string)\n        # Extract the state abbreviation if the pattern is found\n        state_abbr = match.group(1) if match else None\n\n        # State Code\n        # ===========\n        state_code = input_string.split(\"k6Slc6nBda61qZ\")[1]\n        print(f\"'{state_abbr}' = '{state_code}',\")\n\n\n'AK' = '-1b2o=',\n'AL' = '-1cGo=',\n'AR' = '-1dmo=',\n'AZ' = '-1fmo=',\n'CA' = '-3ZWo=',\n'CO' = '-3c2o=',\n'CT' = '-3eGo=',\n'DC' = '-4Z2o=',\n'DD' = '-4aGo=',\n'DE' = '-4aWo=',\n'FL' = '-6cGo=',\n'GA' = '-7ZWo=',\n'GU' = '-7eWo=',\n'HI' = '-8bWo=',\n'IA' = '-9ZWo=',\n'ID' = '-9aGo=',\n'IL' = '-9cGo=',\n'IN' = '-9cmo=',\n'KS' = '-_d2o=',\n'KY' = '-_fWo=',\n'LA' = '_AZWo=',\n'MA' = '_BZWo=',\n'MD' = '_BaGo=',\n'ME' = '_BaWo=',\n'MI' = '_BbWo=',\n'MN' = '_Bcmo=',\n'MO' = '_Bc2o=',\n'MS' = '_Bd2o=',\n'MT' = '_BeGo=',\n'NC' = '_CZ2o=',\n'ND' = '_CaGo=',\n'NE' = '_CaWo=',\n'NH' = '_CbGo=',\n'NJ' = '_Cbmo=',\n'NM' = '_CcWo=',\n'NV' = '_Cemo=',\n'NY' = '_CfWo=',\n'OH' = '_DbGo=',\n'OK' = '_Db2o=',\n'OR' = '_Ddmo=',\n'PA' = '_EZWo=',\n'PR' = '_Edmo=',\n'RI' = '_GbWo=',\n'SC' = '_HZ2o=',\n'SD' = '_HaGo=',\n'TN' = '_Icmo=',\n'TX' = '_IfGo=',\n'UT' = '_JeGo=',\n'VA' = '_KZWo=',\n'None' = '_Jd4a-nA==',\n'VT' = '_KeGo=',\n'WA' = '_LZWo=',\n'WI' = '_LbWo=',\n'WV' = '_Lemo=',\n'WY' = '_LfWo=',",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Process PDR G Report"
    ]
  },
  {
    "objectID": "etl/3.parse_foias.html",
    "href": "etl/3.parse_foias.html",
    "title": "Parse FOIA",
    "section": "",
    "text": "Here is the script that parses the scans.\nimport sys\nimport os\n\n# Add the parent directory of etl to the Python path\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nif project_root not in sys.path:\n    sys.path.append(project_root)\n\nfrom table_parsing.utils import set_paths\nfrom table_parsing.ocr import ocr_text\nfrom table_parsing.screenshots import make_screenshots\nfrom table_parsing.image import make_result\n\n\n\npdf_ls = [\n    # {\"photo_pages\": \"/Users/nick/Documents/projects/GitHub/usda/data/source/montana-foias/2022-APHIS-04193-F_8956/8956_1\", \"result\":\"/Users/nick/Documents/projects/GitHub/usda/data/processed/montana-foia-parsed/8956_1_result.jsonl\"},\n    # {\"photo_pages\": \"/Users/nick/Documents/projects/GitHub/usda/data/source/montana-foias/2022-APHIS-04193-F_9430/2022-APHIS-04193-F_9430_2\", \"result\":\"/Users/nick/Documents/projects/GitHub/usda/data/processed/montana-foia-parsed/9430_2_result.jsonl\"},\n    # {\"photo_pages\": \"/Users/nick/Documents/projects/GitHub/usda/data/source/montana-foias/2022-APHIS-04193-F_9557/9557_1\" , \"result\": \"/Users/nick/Documents/projects/GitHub/usda/data/processed/montana-foia-parsed/9557_1_result.jsonl\"},\n    # {\"photo_pages\": \"/Users/nick/Documents/projects/GitHub/usda/data/source/montana-foias/2022-APHIS-04193-F_9557/9557_2\" , \"result\":\"/Users/nick/Documents/projects/GitHub/usda/data/processed/montana-foia-parsed/9557_2_result.jsonl\"},\n    # {\"photo_pages\": \"/Users/nick/Documents/projects/GitHub/usda/data/source/montana-foias/2022-APHIS-04193-F_9557/9557_3\", \"result\":\"/Users/nick/Documents/projects/GitHub/usda/data/processed/montana-foia-parsed/9557_3_result.jsonl\"},\n    # {\"photo_pages\": \"/Users/nick/Documents/projects/GitHub/usda/data/source/montana-foias/2022-APHIS-04193-F_9557/9557_4\", \"result\":\"/Users/nick/Documents/projects/GitHub/usda/data/processed/montana-foia-parsed/9557_4_result.jsonl\"},\n    # {\"photo_pages\": \"/Users/nick/Documents/projects/GitHub/usda/data/source/montana-foias/2022-APHIS-04193-F_8648/8648_dedupe\", \"result\":\"/Users/nick/Documents/projects/GitHub/usda/data/processed/montana-foia-parsed/8648_dedupe_result.jsonl\"},\n\n\n]\n\nfor pdf_item in pdf_ls:\n    pdf_photos_path = pdf_item[\"photo_pages\"]\n    output_result = pdf_item[\"result\"]\n    print(f\"Starting process for {output_result} \")\n    # Create folders\n\n    json_folder_path, screenshots_folder_path, screenshot_annotated_folder_path, screenshot_jpg_path, screenshot_json_path  = set_paths(pdf_photos_path)\n\n    # OCR the pages \n\n    ocr_text(pdf_photos_path, json_folder_path )\n\n    # Screenshot each row\n\n    make_screenshots(pdf_photos_path,json_folder_path, screenshot_jpg_path )\n\n    # OCR each screenshot\n\n    ocr_text(screenshot_jpg_path, screenshot_json_path)\n\n    # Output file\n\n    make_result(screenshot_jpg_path, screenshot_json_path,output_result,screenshot_annotated_folder_path)\n    print(f\"Finished{output_result} \")\n    print(\"-------------------------\")\n\n\n\n\n\n\n\n\nHere is a presentation that explains the code at a high level.",
    "crumbs": [
      "ETL (Extract Transform and Load)",
      "Parse FOIA"
    ]
  }
]